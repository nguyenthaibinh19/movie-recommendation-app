# src-backend/run.py
import os, time

# ‚úÖ ƒê·∫∑t ENV TR∆Ø·ªöC khi import torch/numpy/faiss ƒë·ªÉ tr√°nh oversubscription & xung ƒë·ªôt OpenMP
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("MKL_SERVICE_FORCE_INTEL", "1")
# N·∫øu c√≤n l·ªói libiomp5md.dll ·ªü m√¥i tr∆∞·ªùng dev, c√≥ th·ªÉ b·∫≠t d√≤ng d∆∞·ªõi (kh√¥ng khuy·∫øn ngh·ªã cho prod)
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

t0 = time.perf_counter()
print("[run] starting‚Ä¶")

# Import waitress tr∆∞·ªõc ƒë·ªÉ ƒëo m·ªëc th·ªùi gian
t1 = time.perf_counter()
from waitress import serve
print(f"[run] import waitress: {time.perf_counter() - t1:.2f}s")

# Import Flask app SAU khi ENV ƒë√£ set
t2 = time.perf_counter()
from app import app   # app.py s·∫Ω lazy-load predict_torch/similar khi c·∫ßn
print(f"[run] import app: {time.perf_counter() - t2:.2f}s")

print(f"[run] total import time: {time.perf_counter() - t0:.2f}s")
print("[run] serving with waitress on http://0.0.0.0:5000  (threads=1)")

# üß∞ N·∫øu ch·ªâ demo local, b·∫°n c√≥ th·ªÉ ƒë·ªïi host='127.0.0.1'
serve(app, host="0.0.0.0", port=5000, threads=1)
