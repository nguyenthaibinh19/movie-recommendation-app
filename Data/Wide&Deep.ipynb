{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67bab4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Loaded MovieLens: 1,000,209 interactions | Users=6,040 | Items=3,706\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp  label  u  i\n",
       "0        1      1193       5  978300760      1  0  0\n",
       "1        1       661       3  978302109      0  0  1\n",
       "2        1       914       3  978301968      0  0  2\n",
       "3        1      3408       4  978300275      1  0  3\n",
       "4        1      2355       5  978824291      1  0  4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1 (ĐÃ SỬA TOÀN BỘ): Imports, Config & Robust Loader cho MovieLens 1M (ratings/users/movies)\n",
    "\n",
    "import os, math, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score\n",
    "\n",
    "# ==== Seed & Device ====\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ==== Đường dẫn dữ liệu (đi theo bản bạn đang dùng) ====\n",
    "DATA_DIR = Path(\"./Dataset\")         # đổi nếu khác (vd: ./Data/Dataset/ml-1m)\n",
    "RATINGS_PATH = DATA_DIR / \"ratings.dat\"\n",
    "USERS_PATH   = DATA_DIR / \"users.dat\"\n",
    "MOVIES_PATH  = DATA_DIR / \"movies.dat\"\n",
    "\n",
    "# (fallback nếu bạn chỉ có .csv)\n",
    "RATINGS_CSV = DATA_DIR / \"ratings.csv\"  # userId,movieId,rating,timestamp (ML-latest)\n",
    "\n",
    "# ==== Hyperparams (giữ nguyên để các cell sau dùng) ====\n",
    "EMBED_DIM   = 32\n",
    "HIDDEN_DIMS = [128, 64]\n",
    "DROPOUT     = 0.2\n",
    "LR          = 1e-3\n",
    "BATCH_SIZE  = 4096\n",
    "EPOCHS      = 5\n",
    "NUM_NEG_TRAIN = 3\n",
    "NUM_NEG_EVAL  = 99\n",
    "TOPK = 10\n",
    "\n",
    "# ==== Hàm đọc .dat chống UnicodeDecodeError ====\n",
    "def read_ml1m_dat(path: Path, names):\n",
    "    \"\"\"\n",
    "    Đọc file .dat ML-1M với sep='::' và fallback encoding để tránh UnicodeDecodeError.\n",
    "    Ưu tiên latin-1/cp1252 cho movies.dat (tiêu đề phim có ký tự đặc biệt).\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Không tìm thấy file: {path}\")\n",
    "\n",
    "    encodings = [\"utf-8\", \"ISO-8859-1\", \"latin-1\", \"cp1252\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(\n",
    "                path,\n",
    "                sep=\"::\",\n",
    "                engine=\"python\",\n",
    "                names=names,\n",
    "                encoding=enc,\n",
    "                encoding_errors=\"strict\",  # đổi \"replace\" nếu muốn tránh lỗi bằng ký tự �\n",
    "            )\n",
    "        except UnicodeDecodeError as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "\n",
    "    # Phương án cuối: thay ký tự lỗi để không crash\n",
    "    print(f\"[WARN] All strict decodes failed for {path}. Using cp1252 with replacement.\")\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        sep=\"::\",\n",
    "        engine=\"python\",\n",
    "        names=names,\n",
    "        encoding=\"cp1252\",\n",
    "        encoding_errors=\"replace\",\n",
    "    )\n",
    "\n",
    "# ==== Đọc dữ liệu ====\n",
    "if RATINGS_PATH.exists() and USERS_PATH.exists() and MOVIES_PATH.exists():\n",
    "    ratings = read_ml1m_dat(RATINGS_PATH, [\"user_id\",\"movie_id\",\"rating\",\"timestamp\"])\n",
    "    users   = read_ml1m_dat(USERS_PATH,   [\"user_id\",\"gender\",\"age\",\"occupation\",\"zip\"])\n",
    "    movies  = read_ml1m_dat(MOVIES_PATH,  [\"movie_id\",\"title\",\"genres\"])\n",
    "\n",
    "    # Merge (giữ sẵn metadata nếu sau này cần mở rộng mô hình)\n",
    "    df_full = ratings.merge(users, on=\"user_id\").merge(movies, on=\"movie_id\")\n",
    "\n",
    "    # Baseline Wide&Deep implicit chỉ cần các cột lõi:\n",
    "    df = df_full[[\"user_id\",\"movie_id\",\"rating\",\"timestamp\"]].copy()\n",
    "\n",
    "elif RATINGS_CSV.exists():  # Fallback: chỉ có ratings.csv (phiên bản MovieLens mới)\n",
    "    df_csv = pd.read_csv(RATINGS_CSV)\n",
    "    df_csv.columns = [c.lower() for c in df_csv.columns]\n",
    "    # yêu cầu: userId, movieId, rating, timestamp\n",
    "    req = {\"userid\",\"movieid\",\"rating\",\"timestamp\"}\n",
    "    assert req.issubset(set(df_csv.columns)), f\"ratings.csv thiếu cột {req - set(df_csv.columns)}\"\n",
    "    df = df_csv.rename(columns={\"userid\":\"user_id\",\"movieid\":\"movie_id\"})[[\"user_id\",\"movie_id\",\"rating\",\"timestamp\"]].copy()\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Không thấy dữ liệu. Đặt ML-1M vào {DATA_DIR} gồm ratings.dat/users.dat/movies.dat \"\n",
    "        f\"hoặc ít nhất ratings.csv (userId,movieId,rating,timestamp).\"\n",
    "    )\n",
    "\n",
    "# ==== Chuẩn hóa kiểu dữ liệu & nhị phân hoá nhãn ====\n",
    "# rating >= 4 => 1 (positive), ngược lại 0\n",
    "df[\"label\"] = (df[\"rating\"] >= 4).astype(np.int64)\n",
    "\n",
    "# ép kiểu timestamp về int64 (nếu là object/float)\n",
    "df[\"timestamp\"] = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\").fillna(0).astype(np.int64)\n",
    "\n",
    "# ==== Map ID rời rạc sang index liên tục ====\n",
    "# (giữ nguyên thứ tự xuất hiện để so sánh công bằng với các mô hình trước)\n",
    "user_ids = df[\"user_id\"].unique()\n",
    "item_ids = df[\"movie_id\"].unique()\n",
    "user2idx = {u:i for i,u in enumerate(user_ids)}\n",
    "item2idx = {m:i for i,m in enumerate(item_ids)}\n",
    "\n",
    "df[\"u\"] = df[\"user_id\"].map(user2idx).astype(np.int64)\n",
    "df[\"i\"] = df[\"movie_id\"].map(item2idx).astype(np.int64)\n",
    "\n",
    "N_USERS = len(user2idx)\n",
    "N_ITEMS = len(item2idx)\n",
    "\n",
    "print(f\"Loaded MovieLens: {len(df):,} interactions | Users={N_USERS:,} | Items={N_ITEMS:,}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfd87b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train=988,129  Valid=6,040  Test=6,040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6038,\n",
       " [(0,\n",
       "   {0,\n",
       "    3,\n",
       "    4,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    22,\n",
       "    23,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52})])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2: Temporal split per-user (last->test, second last->valid, rest->train)\n",
    "from collections import defaultdict\n",
    "\n",
    "def temporal_split_userwise(df: pd.DataFrame):\n",
    "    df = df.sort_values([\"u\",\"timestamp\"])\n",
    "    grp = df.groupby(\"u\")\n",
    "\n",
    "    test_idx, valid_idx, train_idx = [], [], []\n",
    "    for u, g in grp:\n",
    "        idxs = g.index.to_list()\n",
    "        if len(idxs) == 1:\n",
    "            test_idx.append(idxs[-1])\n",
    "        elif len(idxs) == 2:\n",
    "            valid_idx.append(idxs[-2])\n",
    "            test_idx.append(idxs[-1])\n",
    "        else:\n",
    "            train_idx.extend(idxs[:-2])\n",
    "            valid_idx.append(idxs[-2])\n",
    "            test_idx.append(idxs[-1])\n",
    "\n",
    "    df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "    df_valid = df.loc[valid_idx].reset_index(drop=True)\n",
    "    df_test  = df.loc[test_idx ].reset_index(drop=True)\n",
    "    print(f\"Train={len(df_train):,}  Valid={len(df_valid):,}  Test={len(df_test):,}\")\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "df_train, df_valid, df_test = temporal_split_userwise(df)\n",
    "\n",
    "# Tạo tập item positive cho mỗi user (phục vụ negative sampling)\n",
    "user_pos_items = defaultdict(set)\n",
    "for r in df_train.itertuples(index=False):\n",
    "    if r.label == 1:\n",
    "        user_pos_items[r.u].add(r.i)\n",
    "for r in df_valid.itertuples(index=False):\n",
    "    if r.label == 1:\n",
    "        user_pos_items[r.u].add(r.i)\n",
    "\n",
    "len(user_pos_items), list(user_pos_items.items())[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68bea90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568232, 6040, 6040)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Dataset & DataLoader\n",
    "\n",
    "class TrainPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Mỗi __getitem__ trả về 1 positive (u,i,1) + NUM_NEG_TRAIN negatives (u,j,0).\n",
    "    Chỉ dùng positives của df_train để học implicit.\n",
    "    \"\"\"\n",
    "    def __init__(self, df_pos: pd.DataFrame, n_items: int, user_pos: dict, num_neg=3):\n",
    "        self.pos = df_pos[df_pos[\"label\"] == 1][[\"u\",\"i\"]].values\n",
    "        self.n_items = n_items\n",
    "        self.user_pos = user_pos\n",
    "        self.num_neg = num_neg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u, i = self.pos[idx]\n",
    "        us = [u]; is_ = [i]; ys = [1.0]\n",
    "        cnt = 0\n",
    "        banned = self.user_pos[u]\n",
    "        while cnt < self.num_neg:\n",
    "            j = np.random.randint(0, self.n_items)\n",
    "            if j not in banned:\n",
    "                us.append(u); is_.append(j); ys.append(0.0)\n",
    "                cnt += 1\n",
    "        return np.array(us, dtype=np.int64), np.array(is_, dtype=np.int64), np.array(ys, dtype=np.float32)\n",
    "\n",
    "def train_collate(batch):\n",
    "    u = np.concatenate([b[0] for b in batch], axis=0)\n",
    "    i = np.concatenate([b[1] for b in batch], axis=0)\n",
    "    y = np.concatenate([b[2] for b in batch], axis=0)\n",
    "    return torch.from_numpy(u), torch.from_numpy(i), torch.from_numpy(y)\n",
    "\n",
    "class PointwiseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dùng cho valid/test binary (không sample negative).\n",
    "    \"\"\"\n",
    "    def __init__(self, df_xy: pd.DataFrame):\n",
    "        self.u = df_xy[\"u\"].astype(np.int64).values\n",
    "        self.i = df_xy[\"i\"].astype(np.int64).values\n",
    "        self.y = df_xy[\"label\"].astype(np.float32).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.u)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.u[idx], self.i[idx], self.y[idx])\n",
    "\n",
    "def pointwise_collate(batch):\n",
    "    u = torch.tensor([b[0] for b in batch], dtype=torch.long)\n",
    "    i = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    y = torch.tensor([b[2] for b in batch], dtype=torch.float32)\n",
    "    return u, i, y\n",
    "\n",
    "train_ds = TrainPairDataset(df_train, N_ITEMS, user_pos_items, NUM_NEG_TRAIN)\n",
    "valid_ds = PointwiseDataset(df_valid)\n",
    "test_ds  = PointwiseDataset(df_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, collate_fn=train_collate)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=8192,     shuffle=False, num_workers=0, collate_fn=pointwise_collate)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=8192,     shuffle=False, num_workers=0, collate_fn=pointwise_collate)\n",
    "\n",
    "len(train_ds), len(valid_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20cf0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideAndDeep(\n",
       "  (user_emb): Embedding(6040, 32)\n",
       "  (item_emb): Embedding(3706, 32)\n",
       "  (user_bias): Embedding(6040, 1)\n",
       "  (item_bias): Embedding(3706, 1)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Wide&Deep model\n",
    "\n",
    "class WideAndDeep(nn.Module):\n",
    "    \"\"\"\n",
    "    Wide: user_bias + item_bias + global_bias (tuyến tính)\n",
    "    Deep: concat(Emb(u), Emb(i)) -> MLP -> logit\n",
    "    Output: deep_logit + wide_logit (dùng BCEWithLogitsLoss)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_items, embed_dim=32, hidden_dims=(128,64), dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, embed_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, embed_dim)\n",
    "\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        layers = []\n",
    "        in_dim = embed_dim * 2\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(in_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            in_dim = h\n",
    "        layers += [nn.Linear(in_dim, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        ue = self.user_emb(u)\n",
    "        ie = self.item_emb(i)\n",
    "        deep_in = torch.cat([ue, ie], dim=-1)\n",
    "        deep_logit = self.mlp(deep_in).squeeze(-1)\n",
    "        wide_logit = (self.user_bias(u).squeeze(-1) +\n",
    "                      self.item_bias(i).squeeze(-1) +\n",
    "                      self.global_bias)\n",
    "        return deep_logit + wide_logit\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, u, i):\n",
    "        return torch.sigmoid(self.forward(u, i))\n",
    "\n",
    "model = WideAndDeep(N_USERS, N_ITEMS, EMBED_DIM, HIDDEN_DIMS, DROPOUT).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184dfd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Evaluate helpers (AUC, LogLoss, Accuracy)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_binary(model, loader, device=DEVICE):\n",
    "    model.eval()\n",
    "    y_true_all, y_prob_all = [], []\n",
    "    for u, i, y in loader:\n",
    "        u = u.to(device); i = i.to(device); y = y.to(device)\n",
    "        logits = model(u, i)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        y_true_all.append(y.detach().cpu().numpy())\n",
    "        y_prob_all.append(probs.detach().cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_prob = np.concatenate(y_prob_all).clip(1e-7, 1-1e-7)\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "    ll  = log_loss(y_true, y_prob, labels=[0,1])\n",
    "    acc = accuracy_score(y_true, (y_prob >= 0.5).astype(int))\n",
    "    return {\"auc\": auc, \"logloss\": ll, \"acc\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e24da6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 139/139 [00:13<00:00, 10.25it/s, loss=0.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] TrainLoss=0.4357 | Valid AUC=0.6923  Acc=0.6147  LogLoss=0.7545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 139/139 [00:17<00:00,  7.93it/s, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] TrainLoss=0.3650 | Valid AUC=0.6942  Acc=0.6106  LogLoss=0.7575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 139/139 [00:17<00:00,  8.00it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] TrainLoss=0.3630 | Valid AUC=0.6919  Acc=0.6036  LogLoss=0.7621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 139/139 [00:16<00:00,  8.35it/s, loss=0.362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] TrainLoss=0.3618 | Valid AUC=0.6903  Acc=0.6015  LogLoss=0.7665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 139/139 [00:17<00:00,  8.07it/s, loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] TrainLoss=0.3587 | Valid AUC=0.6947  Acc=0.6070  LogLoss=0.7568\n",
      "Loaded best model (Valid AUC=0.6947).\n",
      "Test (binary) — AUC: 0.6912 | Acc: 0.5967 | LogLoss: 0.7859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Training loop + best-by-valid-AUC\n",
    "\n",
    "best_valid_auc = -1.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss, n_samples = 0.0, 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", total=len(train_loader))\n",
    "    for u, i, y in pbar:\n",
    "        u = u.to(DEVICE); i = i.to(DEVICE); y = y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(u, i)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        n_samples += bs\n",
    "        pbar.set_postfix(loss=running_loss/max(1, n_samples))\n",
    "\n",
    "    valid_metrics = evaluate_binary(model, valid_loader)\n",
    "    print(f\"[Epoch {epoch}] TrainLoss={running_loss/n_samples:.4f} | \"\n",
    "          f\"Valid AUC={valid_metrics['auc']:.4f}  \"\n",
    "          f\"Acc={valid_metrics['acc']:.4f}  \"\n",
    "          f\"LogLoss={valid_metrics['logloss']:.4f}\")\n",
    "\n",
    "    if valid_metrics[\"auc\"] > best_valid_auc:\n",
    "        best_valid_auc = valid_metrics[\"auc\"]\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    model.to(DEVICE)\n",
    "    print(f\"Loaded best model (Valid AUC={best_valid_auc:.4f}).\")\n",
    "\n",
    "test_metrics = evaluate_binary(model, test_loader)\n",
    "print(\"Test (binary) — AUC: {auc:.4f} | Acc: {acc:.4f} | LogLoss: {logloss:.4f}\".format(**test_metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce9fcb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Ranking helpers (LOO candidates, Hit@K, NDCG@K)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_loo_candidates(df_test, n_items, user_pos_items, num_neg=99):\n",
    "    \"\"\"\n",
    "    Với mỗi user có positive ở test: lấy (u, i_pos) làm positive và sample num_neg items chưa từng positive.\n",
    "    Trả về dict: u -> (pos_i, [neg_items...])\n",
    "    \"\"\"\n",
    "    df_pos_test = df_test[df_test[\"label\"] == 1]\n",
    "    user_pos_test = {int(r.u): int(r.i) for r in df_pos_test.itertuples(index=False)}\n",
    "\n",
    "    loo = {}\n",
    "    for u, pos_i in user_pos_test.items():\n",
    "        negs, banned = [], set(user_pos_items[u]) | {pos_i}\n",
    "        while len(negs) < num_neg:\n",
    "            j = np.random.randint(0, n_items)\n",
    "            if j not in banned:\n",
    "                negs.append(j)\n",
    "        loo[u] = (pos_i, negs)\n",
    "    return loo\n",
    "\n",
    "def hit_ndcg_at_k(scores, pos_index, k=10):\n",
    "    \"\"\"\n",
    "    scores: np.array score cho danh sách ứng viên; pos_index: vị trí positive trong mảng scores.\n",
    "    \"\"\"\n",
    "    rank = (-scores).argsort()\n",
    "    topk = rank[:k]\n",
    "    hit = 1.0 if pos_index in topk else 0.0\n",
    "    # NDCG: 1/log2(rank_of_positive+2)\n",
    "    pos_rank = np.where(rank == pos_index)[0]\n",
    "    ndcg = 1.0 / math.log2(pos_rank[0] + 2.0) if len(pos_rank) > 0 else 0.0\n",
    "    return hit, ndcg\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_ranking(model, loo_dict, k=10, device=DEVICE):\n",
    "    model.eval()\n",
    "    hits, ndcgs = [], []\n",
    "    for u, (pos_i, negs) in tqdm(loo_dict.items(), desc=\"Ranking eval\"):\n",
    "        cand_items = [pos_i] + negs  # positive ở index 0\n",
    "        u_arr = torch.full((len(cand_items),), u, dtype=torch.long, device=device)\n",
    "        i_arr = torch.tensor(cand_items, dtype=torch.long, device=device)\n",
    "        scores = model.predict_proba(u_arr, i_arr).detach().cpu().numpy()\n",
    "        hit, ndcg = hit_ndcg_at_k(scores, pos_index=0, k=k)\n",
    "        hits.append(hit); ndcgs.append(ndcg)\n",
    "    return {\"hit@k\": float(np.mean(hits)), \"ndcg@k\": float(np.mean(ndcgs))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6631618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in LOO: 3563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking eval: 100%|██████████| 3563/3563 [00:01<00:00, 3505.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking (leave-one-out, K=10) — Hit@10: 0.5535 | NDCG@10: 0.4064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Compute Hit@10 & NDCG@10\n",
    "\n",
    "loo = generate_loo_candidates(df_test, N_ITEMS, user_pos_items, NUM_NEG_EVAL)\n",
    "print(\"Users in LOO:\", len(loo))\n",
    "\n",
    "rank_metrics = evaluate_ranking(model, loo, k=TOPK)\n",
    "print(f\"Ranking (leave-one-out, K={TOPK}) — Hit@{TOPK}: {rank_metrics['hit@k']:.4f} | \"\n",
    "      f\"NDCG@{TOPK}: {rank_metrics['ndcg@k']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Summary print (optional)\n",
    "\n",
    "summary = {\n",
    "    \"valid_auc_best\": float(best_valid_auc),\n",
    "    \"test_auc\": float(test_metrics[\"auc\"]),\n",
    "    \"test_acc\": float(test_metrics[\"acc\"]),\n",
    "    \"test_logloss\": float(test_metrics[\"logloss\"]),\n",
    "    f\"hit@{TOPK}\": float(rank_metrics[\"hit@k\"]),\n",
    "    f\"ndcg@{TOPK}\": float(rank_metrics[\"ndcg@k\"]),\n",
    "}\n",
    "print(\"=== SUMMARY ===\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
