{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21be3695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ====== Config ======\n",
    "DATA_DIR = \"Dataset\"    # chứa users.dat, movies.dat, ratings.dat\n",
    "ART_DIR  = \"artifacts\"       # nơi lưu model & embeddings\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "# Train config\n",
    "SEED            = 42\n",
    "BATCH_SIZE      = 1024\n",
    "EPOCHS          = 4\n",
    "LR              = 1e-3\n",
    "EMBED_DIM       = 16\n",
    "MLP_DIMS        = [128, 64]\n",
    "DROPOUT         = 0.2\n",
    "NEG_PER_POS     = 4      # số negative mỗi positive\n",
    "VAL_SIZE        = 0.1    # 10% validation\n",
    "TEST_SIZE       = 0.1    # 10% test (từ phần còn lại)\n",
    "MAX_SAMPLES_PER_USER = 20   # đặt số nguyên để giới hạn (demo nhanh), None = không giới hạn\n",
    "\n",
    "# ====== Reproducibility ======\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e1e971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id gender  age  occupation    zip\n",
      "0        1      F    1          10  48067\n",
      "1        2      M   56          16  70072\n",
      "   item_id             title                        genres\n",
      "0        1  Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2    Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "   user_id  item_id  rating  timestamp\n",
      "0        1     1193       5  978300760\n",
      "1        1      661       3  978302109\n",
      "Counts: 6040 3883 1000209\n"
     ]
    }
   ],
   "source": [
    "# MovieLens 1M delimiter là '::', encoding latin-1\n",
    "users_path   = os.path.join(DATA_DIR, \"users.dat\")\n",
    "movies_path  = os.path.join(DATA_DIR, \"movies.dat\")\n",
    "ratings_path = os.path.join(DATA_DIR, \"ratings.dat\")\n",
    "\n",
    "# Đọc USERS\n",
    "users = pd.read_csv(\n",
    "    users_path, sep=\"::\", engine=\"python\", encoding=\"latin-1\",\n",
    "    names=[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    ")\n",
    "\n",
    "# Đọc MOVIES\n",
    "movies = pd.read_csv(\n",
    "    movies_path, sep=\"::\", engine=\"python\", encoding=\"latin-1\",\n",
    "    names=[\"item_id\", \"title\", \"genres\"]\n",
    ")\n",
    "\n",
    "# Đọc RATINGS\n",
    "ratings = pd.read_csv(\n",
    "    ratings_path, sep=\"::\", engine=\"python\", encoding=\"latin-1\",\n",
    "    names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "print(users.head(2))\n",
    "print(movies.head(2))\n",
    "print(ratings.head(2))\n",
    "print(\"Counts:\", len(users), len(movies), len(ratings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ed53cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num genres: 18\n",
      "Num items: 3883\n",
      "Sample mappings: [('Action', 0), ('Adventure', 1), ('Animation', 2), (\"Children's\", 3), ('Comedy', 4)] [(1, 0), (2, 1), (3, 2), (4, 3), (5, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Bản đồ giống preprocess.py trong backend của bạn\n",
    "gender_map = {'F': 0, 'M': 1}\n",
    "age_map    = {1:0, 18:1, 25:2, 35:3, 45:4, 50:5, 56:6}   # giữ đúng bucket\n",
    "# occupation đã là số 0..20 -> giữ nguyên\n",
    "\n",
    "users[\"gender_enc\"]     = users[\"gender\"].map(gender_map).astype(int)\n",
    "users[\"age_enc\"]        = users[\"age\"].map(age_map).astype(int)\n",
    "users[\"occupation_enc\"] = users[\"occupation\"].astype(int)\n",
    "\n",
    "# Genre: lấy genre đầu tiên (trùng với predict.py của bạn)\n",
    "def first_genre(s):\n",
    "    if isinstance(s, str) and '|' in s:\n",
    "        return s.split('|')[0]\n",
    "    return s\n",
    "\n",
    "movies[\"genre_first\"] = movies[\"genres\"].apply(first_genre)\n",
    "genre_to_index = {g:i for i, g in enumerate(sorted(movies[\"genre_first\"].unique()))}\n",
    "movies[\"genre_enc\"] = movies[\"genre_first\"].map(genre_to_index).astype(int)\n",
    "\n",
    "# Map item_id -> index liên tục (embedding)\n",
    "unique_item_ids = movies[\"item_id\"].unique()\n",
    "item_id_to_index = {int(i): idx for idx, i in enumerate(sorted(unique_item_ids))}\n",
    "index_to_item_id = {idx: int(i) for i, idx in item_id_to_index.items()}\n",
    "\n",
    "# Lưu mapping CSV như bạn đang dùng (để predict.py vẫn dùng được)\n",
    "map_df = pd.DataFrame({\"item_id\": list(item_id_to_index.keys()),\n",
    "                       \"index\":   list(item_id_to_index.values())})\n",
    "map_df.to_csv(os.path.join(\"item_id_mapping.csv\"), index=False)\n",
    "\n",
    "print(\"Num genres:\", len(genre_to_index))\n",
    "print(\"Num items:\", len(item_id_to_index))\n",
    "print(\"Sample mappings:\", list(genre_to_index.items())[:5], list(item_id_to_index.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ece4b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   user_id  gender_enc  age_enc  occupation_enc  item_idx  genre_enc  label\n",
       " 0        1           0        0              10      1176          7      1\n",
       " 1        1           0        0              10       655          2      0\n",
       " 2        1           0        0              10       902         11      0,\n",
       " label\n",
       " 1    0.575161\n",
       " 0    0.424839\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge ratings với users & movies\n",
    "df = ratings.merge(users[[\"user_id\",\"gender_enc\",\"age_enc\",\"occupation_enc\"]], on=\"user_id\", how=\"left\")\n",
    "df = df.merge(movies[[\"item_id\",\"title\",\"genre_enc\"]], on=\"item_id\", how=\"left\")\n",
    "\n",
    "# Label implicit: rating >= 4 -> 1, else -> 0\n",
    "df[\"label\"] = (df[\"rating\"] >= 4).astype(int)\n",
    "\n",
    "# Map item_id sang index liên tục (embedding)\n",
    "df[\"item_idx\"] = df[\"item_id\"].map(item_id_to_index).astype(int)\n",
    "\n",
    "# Gọn cột cần dùng\n",
    "df = df[[\"user_id\",\"gender_enc\",\"age_enc\",\"occupation_enc\",\"item_idx\",\"genre_enc\",\"label\"]]\n",
    "\n",
    "df.head(3), df[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9909bd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling negatives: 100%|██████████| 6038/6038 [03:14<00:00, 31.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 580405\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>item_idx</th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1176</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1485</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3191</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2690</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2095</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  gender  age  occupation  item_idx  genre  label\n",
       "0        1       0    0          10      1176      7      1\n",
       "1        1       0    0          10      1485     13      0\n",
       "2        1       0    0          10      3191      7      0\n",
       "3        1       0    0          10      2690      4      0\n",
       "4        1       0    0          10      2095      7      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tạo danh sách items user đã tương tác (positive)\n",
    "user_pos_items = df[df[\"label\"]==1].groupby(\"user_id\")[\"item_idx\"].apply(set).to_dict()\n",
    "all_item_indices = set(item_id_to_index.values())\n",
    "\n",
    "def build_training_rows(df_pos, neg_per_pos=4, max_samples_per_user=None):\n",
    "    rows = []\n",
    "    grouped = df_pos.groupby(\"user_id\")\n",
    "    for uid, g in tqdm(grouped, desc=\"Sampling negatives\"):\n",
    "        pos_items = list(g[\"item_idx\"].values)\n",
    "        if max_samples_per_user:\n",
    "            pos_items = pos_items[:max_samples_per_user]\n",
    "        pos_set = set(pos_items)\n",
    "        # negatives = items user chưa like\n",
    "        neg_pool = list(all_item_indices - pos_set)\n",
    "        # đảm bảo có đủ neg\n",
    "        if len(neg_pool) == 0:\n",
    "            continue\n",
    "\n",
    "        # Lấy user features (giả sử ổn định trong g)\n",
    "        gender = int(g[\"gender_enc\"].iloc[0])\n",
    "        age    = int(g[\"age_enc\"].iloc[0])\n",
    "        occ    = int(g[\"occupation_enc\"].iloc[0])\n",
    "\n",
    "        for pos in pos_items:\n",
    "            # Positive row\n",
    "            genre = int(df_pos[(df_pos[\"user_id\"]==uid)&(df_pos[\"item_idx\"]==pos)][\"genre_enc\"].iloc[0])\n",
    "            rows.append([uid, gender, age, occ, pos, genre, 1])\n",
    "\n",
    "            # Negatives\n",
    "            neg_sample = np.random.choice(neg_pool, size=neg_per_pos, replace=False)\n",
    "            for neg in neg_sample:\n",
    "                # genre của neg item (lấy nhanh từ movies)\n",
    "                # để tránh join đắt, chuẩn bị mảng genre_by_item trước\n",
    "                rows.append([uid, gender, age, occ, int(neg), int(genre_by_item[neg]), 0])\n",
    "    return rows\n",
    "\n",
    "# Chuẩn bị mảng genre_by_item để tra cứu nhanh\n",
    "# index -> genre_enc\n",
    "genre_by_item = np.zeros(len(item_id_to_index), dtype=np.int32)\n",
    "for _, r in movies.iterrows():\n",
    "    idx = item_id_to_index[int(r[\"item_id\"])]\n",
    "    genre_by_item[idx] = int(r[\"genre_enc\"])\n",
    "\n",
    "df_pos = df[df[\"label\"]==1]\n",
    "rows = build_training_rows(df_pos, NEG_PER_POS, MAX_SAMPLES_PER_USER)\n",
    "train_df = pd.DataFrame(rows, columns=[\"user_id\",\"gender\",\"age\",\"occupation\",\"item_idx\",\"genre\",\"label\"])\n",
    "print(\"Train rows:\", len(train_df))\n",
    "train_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0df7e4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 470127 52237 58041\n"
     ]
    }
   ],
   "source": [
    "# Split theo hàng (không theo user), đơn giản cho demo\n",
    "train_part, test_part = train_test_split(train_df, test_size=TEST_SIZE, random_state=SEED, stratify=train_df[\"label\"])\n",
    "train_part, val_part  = train_test_split(train_part, test_size=VAL_SIZE, random_state=SEED, stratify=train_part[\"label\"])\n",
    "\n",
    "print(\"Split sizes:\", len(train_part), len(val_part), len(test_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04639415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gender': torch.Size([1024]), 'age': torch.Size([1024]), 'occupation': torch.Size([1024]), 'item_id': torch.Size([1024]), 'genre': torch.Size([1024])} torch.Size([1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(470127, 52237, 58041)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FMTrainDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.gender = df[\"gender\"].values.astype(np.int64)\n",
    "        self.age = df[\"age\"].values.astype(np.int64)\n",
    "        self.occ = df[\"occupation\"].values.astype(np.int64)\n",
    "        self.item = df[\"item_idx\"].values.astype(np.int64)\n",
    "        self.genre = df[\"genre\"].values.astype(np.int64)\n",
    "        self.label = df[\"label\"].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"gender\": self.gender[idx],\n",
    "            \"age\": self.age[idx],\n",
    "            \"occupation\": self.occ[idx],\n",
    "            \"item_id\": self.item[idx],\n",
    "            \"genre\": self.genre[idx],\n",
    "            \"label\": self.label[idx]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    out = {k: [] for k in [\"gender\",\"age\",\"occupation\",\"item_id\",\"genre\",\"label\"]}\n",
    "    for b in batch:\n",
    "        for k in out: out[k].append(b[k])\n",
    "    for k in out: out[k] = torch.tensor(out[k])\n",
    "    labels = out.pop(\"label\").float()\n",
    "    return out, labels\n",
    "\n",
    "train_ds = FMTrainDataset(train_part)\n",
    "val_ds   = FMTrainDataset(val_part)\n",
    "test_ds  = FMTrainDataset(test_part)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=0,                 # <— quan trọng: 0 để tránh treo\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,              # <— tắt để an toàn khi không dùng CUDA\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "b, y = next(iter(train_loader))\n",
    "print({k: b[k].shape for k in b}, y.shape)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, collate_fn=collate_fn, pin_memory=False, persistent_workers=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, collate_fn=collate_fn, pin_memory=False, persistent_workers=False\n",
    ")\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06fa055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, field_dims: dict, embed_dim=16, mlp_dims=[128,64], dropout=0.2):\n",
    "        \"\"\"\n",
    "        field_dims: dict tên_field -> cardinality (số lượng giá trị rời rạc)\n",
    "            ví dụ:\n",
    "            {\n",
    "                \"gender\": 2,\n",
    "                \"age\": 7,\n",
    "                \"occupation\": 21,\n",
    "                \"item_id\": num_items,\n",
    "                \"genre\": num_genres\n",
    "            }\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fields = list(field_dims.keys())\n",
    "\n",
    "        # Embedding cho FM/DNN (kích thước d)\n",
    "        self.emb = nn.ModuleDict({\n",
    "            k: nn.Embedding(field_dims[k], embed_dim) for k in self.fields\n",
    "        })\n",
    "        # Linear term: mỗi field một Embedding dim=1\n",
    "        self.lin = nn.ModuleDict({\n",
    "            k: nn.Embedding(field_dims[k], 1) for k in self.fields\n",
    "        })\n",
    "\n",
    "        # DNN\n",
    "        in_dim = embed_dim * len(self.fields)\n",
    "        layers = []\n",
    "        d = in_dim\n",
    "        for h in mlp_dims:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            d = h\n",
    "        self.dnn = nn.Sequential(*layers)\n",
    "        self.dnn_out = nn.Linear(d, 1)\n",
    "\n",
    "        # init\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for emb in self.emb.values():\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "        for l in self.lin.values():\n",
    "            nn.init.zeros_(l.weight.data)\n",
    "\n",
    "    def forward(self, x: dict):\n",
    "        \"\"\"\n",
    "        x: dict tensor Long: [\"gender\",\"age\",\"occupation\",\"item_id\",\"genre\"] shape [B]\n",
    "        Output: logits [B, 1]\n",
    "        \"\"\"\n",
    "        # Linear\n",
    "        lin_terms = [self.lin[k](x[k]) for k in self.fields]  # list [B,1]\n",
    "        lin = torch.stack(lin_terms, dim=1).sum(dim=1)        # [B,1]\n",
    "\n",
    "        # Embeddings\n",
    "        embs = [self.emb[k](x[k]) for k in self.fields]       # list [B,d]\n",
    "        E = torch.stack(embs, dim=1)                          # [B,F,d]\n",
    "\n",
    "        # FM 2nd order: 0.5 * (sum^2 - sum of squares)\n",
    "        sum_of_emb = E.sum(dim=1)                             # [B,d]\n",
    "        sum_of_emb_square = sum_of_emb * sum_of_emb           # [B,d]\n",
    "        square_of_emb = E * E                                 # [B,F,d]\n",
    "        square_of_emb_sum = square_of_emb.sum(dim=1)          # [B,d]\n",
    "        fm = 0.5 * (sum_of_emb_square - square_of_emb_sum)    # [B,d]\n",
    "        fm_logit = fm.sum(dim=1, keepdim=True)                # [B,1]\n",
    "\n",
    "        # DNN\n",
    "        dnn_in = torch.cat(embs, dim=1)                       # [B, F*d]\n",
    "        dnn_hidden = self.dnn(dnn_in)                         # [B, H]\n",
    "        dnn_logit = self.dnn_out(dnn_hidden)                  # [B,1]\n",
    "\n",
    "        logits = lin + fm_logit + dnn_logit                   # [B,1]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec915051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.085516, 'M params')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_items  = len(item_id_to_index)\n",
    "num_genres = len(genre_to_index)\n",
    "\n",
    "field_dims = {\n",
    "    \"gender\": 2,\n",
    "    \"age\": 7,\n",
    "    \"occupation\": 21,\n",
    "    \"item_id\": num_items,\n",
    "    \"genre\": num_genres\n",
    "}\n",
    "\n",
    "model = DeepFM(field_dims, embed_dim=EMBED_DIM, mlp_dims=MLP_DIMS, dropout=DROPOUT).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())/1e6, \"M params\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64a9e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    for batch, labels in loader:\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        logits = model(batch).squeeze(1)  # [B]\n",
    "        all_logits.append(logits.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "    logits = np.concatenate(all_logits)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    probs = 1/(1+np.exp(-logits))\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "    ll = log_loss(labels, probs, labels=[0,1])\n",
    "    return auc, ll\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for bi, (batch, labels) in enumerate(tqdm(loader, total=len(loader), desc=\"Train\")):\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch).squeeze(1)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        if (bi + 1) % 50 == 0:\n",
    "            print(f\"  step {bi+1}/{len(loader)}  loss={loss.item():.4f}\")\n",
    "    return running_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e8bb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  14%|█▍        | 65/460 [00:01<00:04, 87.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 50/460  loss=0.5261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  24%|██▎       | 109/460 [00:01<00:03, 99.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 100/460  loss=0.3614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  36%|███▌      | 164/460 [00:02<00:02, 99.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 150/460  loss=0.3313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  47%|████▋     | 214/460 [00:02<00:02, 98.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 200/460  loss=0.2943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  57%|█████▋    | 264/460 [00:03<00:02, 96.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 250/460  loss=0.3359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  68%|██████▊   | 314/460 [00:03<00:01, 95.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 300/460  loss=0.2512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  78%|███████▊  | 358/460 [00:04<00:01, 99.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 350/460  loss=0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  90%|████████▉ | 412/460 [00:04<00:00, 100.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 400/460  loss=0.2851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 460/460 [00:05<00:00, 89.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 450/460  loss=0.2751\n",
      "[Epoch 01] TrainLoss=0.3364 | ValAUC=0.9216 | ValLogLoss=0.2732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  13%|█▎        | 61/460 [00:00<00:03, 100.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 50/460  loss=0.2822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  25%|██▌       | 115/460 [00:01<00:03, 101.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 100/460  loss=0.3053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  35%|███▍      | 159/460 [00:01<00:02, 101.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 150/460  loss=0.2538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  47%|████▋     | 214/460 [00:02<00:02, 102.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 200/460  loss=0.2532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  58%|█████▊    | 268/460 [00:02<00:01, 100.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 250/460  loss=0.2880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  68%|██████▊   | 312/460 [00:03<00:01, 99.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 300/460  loss=0.2664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  79%|███████▊  | 362/460 [00:03<00:01, 97.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 350/460  loss=0.2551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  91%|█████████ | 417/460 [00:04<00:00, 99.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 400/460  loss=0.2564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 460/460 [00:04<00:00, 99.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 450/460  loss=0.2720\n",
      "[Epoch 02] TrainLoss=0.2719 | ValAUC=0.9233 | ValLogLoss=0.2705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  14%|█▎        | 63/460 [00:00<00:03, 100.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 50/460  loss=0.2777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  26%|██▌       | 118/460 [00:01<00:03, 101.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 100/460  loss=0.2607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  35%|███▌      | 162/460 [00:01<00:02, 102.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 150/460  loss=0.2529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  47%|████▋     | 217/460 [00:02<00:02, 100.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 200/460  loss=0.2532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  56%|█████▌    | 258/460 [00:02<00:02, 93.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 250/460  loss=0.2639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  67%|██████▋   | 309/460 [00:03<00:01, 97.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 300/460  loss=0.2879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  78%|███████▊  | 360/460 [00:03<00:01, 95.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 350/460  loss=0.2650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  89%|████████▉ | 410/460 [00:04<00:00, 89.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 400/460  loss=0.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 460/460 [00:04<00:00, 95.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 450/460  loss=0.2393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03] TrainLoss=0.2680 | ValAUC=0.9244 | ValLogLoss=0.2691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  15%|█▌        | 69/460 [00:00<00:04, 94.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 50/460  loss=0.2556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  24%|██▍       | 111/460 [00:01<00:03, 94.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 100/460  loss=0.2943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  37%|███▋      | 169/460 [00:01<00:03, 90.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 150/460  loss=0.2880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  45%|████▌     | 209/460 [00:02<00:02, 93.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 200/460  loss=0.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  57%|█████▋    | 260/460 [00:02<00:02, 96.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 250/460  loss=0.2781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  67%|██████▋   | 310/460 [00:03<00:01, 94.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 300/460  loss=0.2933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  78%|███████▊  | 361/460 [00:03<00:01, 96.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 350/460  loss=0.2589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  90%|█████████ | 414/460 [00:04<00:00, 99.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 400/460  loss=0.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 460/460 [00:04<00:00, 93.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 450/460  loss=0.2559\n",
      "[Epoch 04] TrainLoss=0.2642 | ValAUC=0.9256 | ValLogLoss=0.2674\n",
      "[TEST] AUC=0.9240 | LogLoss=0.2701\n"
     ]
    }
   ],
   "source": [
    "best_val_auc = -1\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_auc, val_ll = evaluate(model, val_loader)\n",
    "    print(f\"[Epoch {epoch:02d}] TrainLoss={tr_loss:.4f} | ValAUC={val_auc:.4f} | ValLogLoss={val_ll:.4f}\")\n",
    "\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "# Load best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "test_auc, test_ll = evaluate(model, test_loader)\n",
    "print(f\"[TEST] AUC={test_auc:.4f} | LogLoss={test_ll:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a92c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Saved: artifacts\\deepfm_pytorch.pth\n",
      "[✓] Saved: artifacts\\item_emb.npy (3883, 16)\n",
      "[✓] Saved mappings to artifacts/\n"
     ]
    }
   ],
   "source": [
    "# 1) Lưu model (state_dict)\n",
    "pth_path = os.path.join(ART_DIR, \"deepfm_pytorch.pth\")\n",
    "torch.save(model.state_dict(), pth_path)\n",
    "print(\"[✓] Saved:\", pth_path)\n",
    "\n",
    "# 2) Xuất item embedding để dùng FAISS & /similar_items\n",
    "item_emb_weight = model.emb[\"item_id\"].weight.detach().cpu().numpy().astype(\"float32\")\n",
    "np.save(os.path.join(ART_DIR, \"item_emb.npy\"), item_emb_weight)\n",
    "print(\"[✓] Saved:\", os.path.join(ART_DIR, \"item_emb.npy\"), item_emb_weight.shape)\n",
    "\n",
    "# 3) Lưu mapping\n",
    "with open(os.path.join(ART_DIR, \"item_id_to_index.json\"), \"w\") as f:\n",
    "    json.dump({int(k): int(v) for k, v in item_id_to_index.items()}, f)\n",
    "with open(os.path.join(ART_DIR, \"index_to_item_id.json\"), \"w\") as f:\n",
    "    json.dump({int(k): int(v) for k, v in index_to_item_id.items()}, f)\n",
    "print(\"[✓] Saved mappings to artifacts/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed8a1b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 item_idx: [1542  370 3523 1402  870  876  800 1657 3817 1508]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>374</td>\n",
       "      <td>Richie Rich (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>810</td>\n",
       "      <td>Kazaam (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>881</td>\n",
       "      <td>First Kid (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>888</td>\n",
       "      <td>Land Before Time III: The Time of the Great Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>1426</td>\n",
       "      <td>Zeus and Roxanne (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>1547</td>\n",
       "      <td>Shiloh (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>1583</td>\n",
       "      <td>Simple Wish, A (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>1705</td>\n",
       "      <td>Guy (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3523</th>\n",
       "      <td>3592</td>\n",
       "      <td>Time Masters (Les Maîtres du Temps) (1982)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3817</th>\n",
       "      <td>3887</td>\n",
       "      <td>Went to Coney Island on a Mission From God... ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id                                              title\n",
       "370       374                                 Richie Rich (1994)\n",
       "800       810                                      Kazaam (1996)\n",
       "870       881                                   First Kid (1996)\n",
       "876       888  Land Before Time III: The Time of the Great Gi...\n",
       "1402     1426                            Zeus and Roxanne (1997)\n",
       "1508     1547                                      Shiloh (1997)\n",
       "1542     1583                              Simple Wish, A (1997)\n",
       "1657     1705                                         Guy (1996)\n",
       "3523     3592         Time Masters (Les Maîtres du Temps) (1982)\n",
       "3817     3887  Went to Coney Island on a Mission From God... ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ví dụ: tạo vector u giả (mean của một vài item hành động) rồi chấm điểm\n",
    "# Trong thực tế bạn sẽ tối ưu u từ lịch sử user hoặc cập nhật online theo event.\n",
    "\n",
    "# Lấy 10 item ngẫu nhiên\n",
    "sample_idx = np.random.choice(len(item_id_to_index), size=10, replace=False)\n",
    "E = item_emb_weight[sample_idx]  # [10, d]\n",
    "u = E.mean(axis=0)               # [d]\n",
    "u = u / (np.linalg.norm(u) + 1e-6)\n",
    "\n",
    "scores = item_emb_weight @ u     # [num_items]\n",
    "topk = 10\n",
    "top_idx = np.argpartition(-scores, topk)[:topk]\n",
    "top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "print(\"Top-10 item_idx:\", top_idx)\n",
    "\n",
    "# map sang item_id & title\n",
    "inv_map = {v:k for k, v in item_id_to_index.items()}\n",
    "top_item_ids = [inv_map[i] for i in top_idx]\n",
    "top_titles = movies[movies[\"item_id\"].isin(top_item_ids)][[\"item_id\",\"title\"]]\n",
    "top_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55ae8946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'item_id': 3886,\n",
       "  'title': 'Steal This Movie! (2000)',\n",
       "  'score': 0.9638875722885132},\n",
       " {'item_id': 3636,\n",
       "  'title': \"Those Who Love Me Can Take the Train (Ceux qui m'aiment prendront le train) (1998)\",\n",
       "  'score': 0.9467151165008545},\n",
       " {'item_id': 2165,\n",
       "  'title': 'Your Friends and Neighbors (1998)',\n",
       "  'score': 0.9466490745544434},\n",
       " {'item_id': 2131,\n",
       "  'title': 'Autumn Sonata (Höstsonaten ) (1978)',\n",
       "  'score': 0.9396710395812988},\n",
       " {'item_id': 882,\n",
       "  'title': 'Trigger Effect, The (1996)',\n",
       "  'score': 0.9308792352676392},\n",
       " {'item_id': 3619,\n",
       "  'title': 'Hollywood Knights, The (1980)',\n",
       "  'score': 0.9290317296981812},\n",
       " {'item_id': 872,\n",
       "  'title': 'Aiqing wansui (1994)',\n",
       "  'score': 0.9279145002365112},\n",
       " {'item_id': 2904, 'title': 'Rain (1932)', 'score': 0.9272639751434326},\n",
       " {'item_id': 3437, 'title': 'Cool as Ice (1991)', 'score': 0.9268709421157837},\n",
       " {'item_id': 713,\n",
       "  'title': 'Of Love and Shadows (1994)',\n",
       "  'score': 0.9261972308158875}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chỉ cần nếu bạn muốn test Similar ngay trong notebook\n",
    "# !pip install faiss-cpu\n",
    "\n",
    "import faiss\n",
    "emb = item_emb_weight.copy()\n",
    "faiss.normalize_L2(emb)\n",
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "\n",
    "def similar_items_by_item_id(raw_item_id, k=10):\n",
    "    idx = item_id_to_index[int(raw_item_id)]\n",
    "    q = emb[idx:idx+1]\n",
    "    D, I = index.search(q, k+1)\n",
    "    I = I[0].tolist()\n",
    "    D = D[0].tolist()\n",
    "    out = []\n",
    "    for d, i in zip(D, I):\n",
    "        if i == idx: \n",
    "            continue\n",
    "        iid = index_to_item_id[i]\n",
    "        title = movies.loc[movies[\"item_id\"]==iid, \"title\"].values\n",
    "        title = title[0] if len(title) else str(iid)\n",
    "        out.append({\"item_id\": int(iid), \"title\": title, \"score\": float(d)})\n",
    "        if len(out) == k: break\n",
    "    return out\n",
    "\n",
    "# Demo với 1 item bất kỳ:\n",
    "some_item = int(movies[\"item_id\"].sample(1, random_state=SEED).iloc[0])\n",
    "similar_items_by_item_id(some_item, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48de8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helpers + Patch (FINAL) — dùng đúng model.fields ===\n",
    "import numpy as np, torch\n",
    "\n",
    "# Pool item và map user->set item đã like (rating >= 4)\n",
    "ALL_ITEM_IDS = np.array(sorted(movies[\"item_id\"].unique()))\n",
    "watched_by_user = ratings[ratings[\"rating\"] >= 4].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "\n",
    "def sample_negatives_for_user(user_id: int, num_neg: int = 99):\n",
    "    pos_set = watched_by_user.get(user_id, set())\n",
    "    pool = (ALL_ITEM_IDS if len(pos_set) == 0\n",
    "            else np.setdiff1d(ALL_ITEM_IDS, np.fromiter(pos_set, dtype=int), assume_unique=True))\n",
    "    if len(pool) == 0: pool = ALL_ITEM_IDS\n",
    "    replace = len(pool) < num_neg\n",
    "    return np.random.choice(pool, size=num_neg, replace=replace).tolist()\n",
    "\n",
    "def _tensors_for_user_items(user_id: int, item_ids: list[int]):\n",
    "    \"\"\"Chuẩn bị (u,i,g,a,o,ge) tensors theo đúng thứ tự item_ids.\"\"\"\n",
    "    # user feats\n",
    "    urow = users.loc[users[\"user_id\"] == user_id, [\"gender_enc\",\"age_enc\",\"occupation_enc\"]]\n",
    "    if urow.empty:\n",
    "        raise ValueError(f\"User {user_id} không tồn tại trong users df\")\n",
    "    g = int(urow[\"gender_enc\"].values[0])\n",
    "    a = int(urow[\"age_enc\"].values[0])\n",
    "    o = int(urow[\"occupation_enc\"].values[0])\n",
    "\n",
    "    # item feats (theo đúng thứ tự item_ids)\n",
    "    msub = movies.loc[movies[\"item_id\"].isin(item_ids), [\"item_id\",\"genre_enc\"]].copy()\n",
    "    msub = msub.set_index(\"item_id\").reindex(item_ids).reset_index()\n",
    "    item_idx = [item_id_to_index[int(i)] for i in msub[\"item_id\"].values]\n",
    "    genre_enc = msub[\"genre_enc\"].astype(int).values\n",
    "\n",
    "    n = len(item_ids)\n",
    "    uT = torch.full((n,), int(user_id), dtype=torch.long, device=DEVICE)\n",
    "    iT = torch.tensor(item_idx, dtype=torch.long, device=DEVICE)\n",
    "    gT = torch.full((n,), g, dtype=torch.long, device=DEVICE)\n",
    "    aT = torch.full((n,), a, dtype=torch.long, device=DEVICE)\n",
    "    oT = torch.full((n,), o, dtype=torch.long, device=DEVICE)\n",
    "    geT= torch.tensor(genre_enc, dtype=torch.long, device=DEVICE)\n",
    "    return uT, iT, gT, aT, oT, geT\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_candidates(user_id: int, item_ids: list[int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Gọi model theo đúng chữ ký dict dựa vào model.fields.\n",
    "    Ví dụ nếu model.fields = [\"gender\",\"age\",\"occupation\",\"item_id\",\"genre\"],\n",
    "    ta sẽ cung cấp một dict với đúng các khóa đó.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    uT, iT, gT, aT, oT, geT = _tensors_for_user_items(user_id, item_ids)\n",
    "\n",
    "    # Lấy danh sách khóa model yêu cầu\n",
    "    fields = getattr(model, \"fields\", None)\n",
    "    if fields is None:\n",
    "        # Fallback phổ biến: dùng tên khóa tiêu chuẩn\n",
    "        fields = [\"user_id\",\"item_id\",\"gender\",\"age\",\"occupation\",\"genre\"]\n",
    "\n",
    "    batch = {}\n",
    "    for k in fields:\n",
    "        lk = k.lower()\n",
    "        if lk in (\"user\", \"user_id\", \"uid\"):\n",
    "            batch[k] = uT\n",
    "        elif lk in (\"item\", \"item_id\", \"item_idx\", \"iid\"):\n",
    "            # DÙ tên là item_id nhưng thực chất model thường expect chỉ số embedding (item_idx)\n",
    "            batch[k] = iT\n",
    "        elif lk in (\"gender\", \"sex\"):\n",
    "            batch[k] = gT\n",
    "        elif lk == \"age\":\n",
    "            batch[k] = aT\n",
    "        elif lk in (\"occupation\", \"occ\", \"job\"):\n",
    "            batch[k] = oT\n",
    "        elif lk in (\"genre\", \"genre_enc\"):\n",
    "            batch[k] = geT\n",
    "        else:\n",
    "            raise KeyError(f\"Không biết map khóa '{k}' trong model.fields -> cung cấp tensor nào\")\n",
    "\n",
    "    out = model(batch)          # forward(self, x: dict)\n",
    "    # Chuẩn về numpy 1D\n",
    "    out = out.detach().float().view(-1).cpu().numpy()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8e9260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_21580\\1679843186.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_pos = test_pos.groupby(\"user_id\", group_keys=False).apply(\n",
      "Eval Hit@10/NDCG@10: 100%|██████████| 5213/5213 [00:27<00:00, 187.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Users evaluated: 5213\n",
      "[TEST] Hit@10 = 0.7506 | NDCG@10 = 0.4916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate Hit@K & NDCG@K (REPLACED; place after Cell 15) ===\n",
    "# Nhớ chạy Cell 11 (load best model) trước cell này.\n",
    "\n",
    "K = TOPK if \"TOPK\" in globals() else 10\n",
    "NEG_PER_USER_EVAL = 99\n",
    "\n",
    "# Dùng test_df nếu đã alias ở Cell 15a; nếu không có thì dùng test_part\n",
    "_eval_source = test_part if \"test_df\" in globals() else test_part\n",
    "\n",
    "# Xác định cột item: ưu tiên item_id; nếu không có thì dùng item_idx và map lại\n",
    "if \"item_id\" in _eval_source.columns:\n",
    "    ITEM_COL = \"item_id\"\n",
    "elif \"item_idx\" in _eval_source.columns:\n",
    "    ITEM_COL = \"item_idx\"\n",
    "else:\n",
    "    raise RuntimeError(\"Không thấy cột item_id hoặc item_idx trong test set.\")\n",
    "\n",
    "# Lọc positive và lấy ngẫu nhiên 1 positive / user\n",
    "test_pos = _eval_source[_eval_source[\"label\"] == 1].copy()\n",
    "if len(test_pos) == 0:\n",
    "    raise RuntimeError(\"Không tìm thấy positive nào trong test set để đánh giá Hit@K/NDCG@K.\")\n",
    "\n",
    "test_pos = test_pos.groupby(\"user_id\", group_keys=False).apply(\n",
    "    lambda x: x.sample(n=1, random_state=SEED)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "hits, ndcgs = [], []\n",
    "\n",
    "# Dùng iterrows để truy cập theo tên cột ổn định\n",
    "for _, row in tqdm(test_pos.iterrows(), total=len(test_pos), desc=f\"Eval Hit@{K}/NDCG@{K}\"):\n",
    "    u = int(row[\"user_id\"])\n",
    "    if ITEM_COL == \"item_id\":\n",
    "        pos_item = int(row[\"item_id\"])\n",
    "    else:\n",
    "        # map item_idx -> item_id gốc\n",
    "        pos_item = int(index_to_item_id[int(row[\"item_idx\"])])\n",
    "\n",
    "    neg_items = sample_negatives_for_user(u, num_neg=NEG_PER_USER_EVAL)\n",
    "\n",
    "    # 1 positive + negatives (loại trùng)\n",
    "    candidates = [pos_item] + [it for it in neg_items if it != pos_item]\n",
    "    scores = score_candidates(u, candidates)\n",
    "\n",
    "    # Xếp hạng giảm dần\n",
    "    order = np.argsort(-scores)\n",
    "    ranked_items = np.array(candidates, dtype=int)[order]\n",
    "\n",
    "    # Hit@K\n",
    "    topk_items = ranked_items[:K]\n",
    "    hit = 1.0 if pos_item in topk_items else 0.0\n",
    "\n",
    "    # NDCG@K (IDCG=1). Nếu pos ngoài top-K => 0\n",
    "    r_idx = np.where(ranked_items == pos_item)[0]\n",
    "    if len(r_idx) == 0:\n",
    "        dcg_at_k = 0.0\n",
    "    else:\n",
    "        r = int(r_idx[0]) + 1  # 1-based\n",
    "        dcg_at_k = (1.0 / np.log2(r + 1)) if r <= K else 0.0\n",
    "    ndcg = dcg_at_k\n",
    "\n",
    "    hits.append(hit)\n",
    "    ndcgs.append(ndcg)\n",
    "\n",
    "print(f\"[TEST] Users evaluated: {len(test_pos)}\")\n",
    "print(f\"[TEST] Hit@{K} = {np.mean(hits):.4f} | NDCG@{K} = {np.mean(ndcgs):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf215",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
