{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21be3695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ====== Config ======\n",
    "DATA_DIR = \"Dataset\"    # chứa users.dat, movies.dat, ratings.dat\n",
    "ART_DIR  = \"artifacts\"       # nơi lưu model & embeddings\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "# Train config\n",
    "SEED            = 42\n",
    "BATCH_SIZE      = 1024\n",
    "EPOCHS          = 4\n",
    "LR              = 1e-3\n",
    "EMBED_DIM       = 16\n",
    "MLP_DIMS        = [128, 64]\n",
    "DROPOUT         = 0.2\n",
    "NEG_PER_POS     = 4      # số negative mỗi positive\n",
    "VAL_SIZE        = 0.1    # 10% validation\n",
    "TEST_SIZE       = 0.1    # 10% test (từ phần còn lại)\n",
    "MAX_SAMPLES_PER_USER = 20   # đặt số nguyên để giới hạn (demo nhanh), None = không giới hạn\n",
    "# Popularity debias training\n",
    "POP_BETA = 0.6          # for reweighting (Fix 4), recommended 0.3–0.8\n",
    "NEG_POP_ALPHA = 0.75    # for popularity-based negative sampling distribution (Fix 5)\n",
    "NEG_POP_MIX = 0.5       # 50% popularity negatives + 50% random negatives\n",
    "\n",
    "# ====== Reproducibility ======\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84e1e971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id gender  age  occupation    zip\n",
      "0        1      F    1          10  48067\n",
      "1        2      M   56          16  70072\n",
      "   item_id             title                        genres\n",
      "0        1  Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2    Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "   user_id  item_id  rating  timestamp\n",
      "0        1     1193       5  978300760\n",
      "1        1      661       3  978302109\n",
      "Counts: 6040 3883 1000209\n"
     ]
    }
   ],
   "source": [
    "# MovieLens 1M delimiter là '::', encoding latin-1\n",
    "users_path   = os.path.join(DATA_DIR, \"users.dat\")\n",
    "movies_path  = os.path.join(DATA_DIR, \"movies.dat\")\n",
    "ratings_path = os.path.join(DATA_DIR, \"ratings.dat\")\n",
    "\n",
    "# Đọc USERS\n",
    "users = pd.read_csv(\n",
    "    users_path, sep=\"::\", engine=\"python\", encoding=\"latin-1\",\n",
    "    names=[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    ")\n",
    "\n",
    "# Đọc MOVIES\n",
    "movies = pd.read_csv(\n",
    "    movies_path, sep=\"::\", engine=\"python\", encoding=\"latin-1\",\n",
    "    names=[\"item_id\", \"title\", \"genres\"]\n",
    ")\n",
    "\n",
    "# Đọc RATINGS\n",
    "ratings = pd.read_csv(\n",
    "    ratings_path, sep=\"::\", engine=\"python\", encoding=\"latin-1\",\n",
    "    names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "print(users.head(2))\n",
    "print(movies.head(2))\n",
    "print(ratings.head(2))\n",
    "print(\"Counts:\", len(users), len(movies), len(ratings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03ed53cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num genres: 18\n",
      "Num items: 3883\n",
      "Sample mappings: [('Action', 0), ('Adventure', 1), ('Animation', 2), (\"Children's\", 3), ('Comedy', 4)] [(1, 0), (2, 1), (3, 2), (4, 3), (5, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Bản đồ giống preprocess.py trong backend của bạn\n",
    "gender_map = {'F': 0, 'M': 1}\n",
    "age_map    = {1:0, 18:1, 25:2, 35:3, 45:4, 50:5, 56:6}   # giữ đúng bucket\n",
    "# occupation đã là số 0..20 -> giữ nguyên\n",
    "\n",
    "users[\"gender_enc\"]     = users[\"gender\"].map(gender_map).astype(int)\n",
    "users[\"age_enc\"]        = users[\"age\"].map(age_map).astype(int)\n",
    "users[\"occupation_enc\"] = users[\"occupation\"].astype(int)\n",
    "\n",
    "# Genre: lấy genre đầu tiên (trùng với predict.py của bạn)\n",
    "def first_genre(s):\n",
    "    if isinstance(s, str) and '|' in s:\n",
    "        return s.split('|')[0]\n",
    "    return s\n",
    "\n",
    "movies[\"genre_first\"] = movies[\"genres\"].apply(first_genre)\n",
    "genre_to_index = {g:i for i, g in enumerate(sorted(movies[\"genre_first\"].unique()))}\n",
    "movies[\"genre_enc\"] = movies[\"genre_first\"].map(genre_to_index).astype(int)\n",
    "\n",
    "# Map item_id -> index liên tục (embedding)\n",
    "unique_item_ids = movies[\"item_id\"].unique()\n",
    "item_id_to_index = {int(i): idx for idx, i in enumerate(sorted(unique_item_ids))}\n",
    "index_to_item_id = {idx: int(i) for i, idx in item_id_to_index.items()}\n",
    "\n",
    "# Lưu mapping CSV như bạn đang dùng (để predict.py vẫn dùng được)\n",
    "map_df = pd.DataFrame({\"item_id\": list(item_id_to_index.keys()),\n",
    "                       \"index\":   list(item_id_to_index.values())})\n",
    "map_df.to_csv(os.path.join(\"item_id_mapping.csv\"), index=False)\n",
    "\n",
    "print(\"Num genres:\", len(genre_to_index))\n",
    "print(\"Num items:\", len(item_id_to_index))\n",
    "print(\"Sample mappings:\", list(genre_to_index.items())[:5], list(item_id_to_index.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ece4b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   user_id  gender_enc  age_enc  occupation_enc  item_idx  genre_enc  label\n",
       " 0        1           0        0              10      1176          7      1\n",
       " 1        1           0        0              10       655          2      0\n",
       " 2        1           0        0              10       902         11      0,\n",
       " label\n",
       " 1    0.575161\n",
       " 0    0.424839\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge ratings với users & movies\n",
    "df = ratings.merge(users[[\"user_id\",\"gender_enc\",\"age_enc\",\"occupation_enc\"]], on=\"user_id\", how=\"left\")\n",
    "df = df.merge(movies[[\"item_id\",\"title\",\"genre_enc\"]], on=\"item_id\", how=\"left\")\n",
    "\n",
    "# Label implicit: rating >= 4 -> 1, else -> 0\n",
    "df[\"label\"] = (df[\"rating\"] >= 4).astype(int)\n",
    "\n",
    "# Map item_id sang index liên tục (embedding)\n",
    "df[\"item_idx\"] = df[\"item_id\"].map(item_id_to_index).astype(int)\n",
    "\n",
    "# Gọn cột cần dùng\n",
    "df = df[[\"user_id\",\"gender_enc\",\"age_enc\",\"occupation_enc\",\"item_idx\",\"genre_enc\",\"label\"]]\n",
    "\n",
    "df.head(3), df[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d2362e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos rate: 575281 / 1000209 = 0.5751607913945985\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pos_count</th>\n",
       "      <th>share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American Beauty (1999)</td>\n",
       "      <td>2853</td>\n",
       "      <td>0.004959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n",
       "      <td>2622</td>\n",
       "      <td>0.004558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n",
       "      <td>2510</td>\n",
       "      <td>0.004363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saving Private Ryan (1998)</td>\n",
       "      <td>2260</td>\n",
       "      <td>0.003929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Raiders of the Lost Ark (1981)</td>\n",
       "      <td>2260</td>\n",
       "      <td>0.003929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Silence of the Lambs, The (1991)</td>\n",
       "      <td>2252</td>\n",
       "      <td>0.003915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Matrix, The (1999)</td>\n",
       "      <td>2171</td>\n",
       "      <td>0.003774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sixth Sense, The (1999)</td>\n",
       "      <td>2163</td>\n",
       "      <td>0.003760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Star Wars: Episode VI - Return of the Jedi (1983)</td>\n",
       "      <td>2127</td>\n",
       "      <td>0.003697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fargo (1996)</td>\n",
       "      <td>2074</td>\n",
       "      <td>0.003605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Schindler's List (1993)</td>\n",
       "      <td>2071</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shawshank Redemption, The (1994)</td>\n",
       "      <td>2046</td>\n",
       "      <td>0.003557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Terminator 2: Judgment Day (1991)</td>\n",
       "      <td>2044</td>\n",
       "      <td>0.003553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Godfather, The (1972)</td>\n",
       "      <td>1989</td>\n",
       "      <td>0.003457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Braveheart (1995)</td>\n",
       "      <td>1977</td>\n",
       "      <td>0.003437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Princess Bride, The (1987)</td>\n",
       "      <td>1924</td>\n",
       "      <td>0.003344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Back to the Future (1985)</td>\n",
       "      <td>1910</td>\n",
       "      <td>0.003320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Shakespeare in Love (1998)</td>\n",
       "      <td>1877</td>\n",
       "      <td>0.003263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>L.A. Confidential (1997)</td>\n",
       "      <td>1876</td>\n",
       "      <td>0.003261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pulp Fiction (1994)</td>\n",
       "      <td>1770</td>\n",
       "      <td>0.003077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  pos_count     share\n",
       "0                              American Beauty (1999)       2853  0.004959\n",
       "1           Star Wars: Episode IV - A New Hope (1977)       2622  0.004558\n",
       "2   Star Wars: Episode V - The Empire Strikes Back...       2510  0.004363\n",
       "3                          Saving Private Ryan (1998)       2260  0.003929\n",
       "4                      Raiders of the Lost Ark (1981)       2260  0.003929\n",
       "5                    Silence of the Lambs, The (1991)       2252  0.003915\n",
       "6                                  Matrix, The (1999)       2171  0.003774\n",
       "7                             Sixth Sense, The (1999)       2163  0.003760\n",
       "8   Star Wars: Episode VI - Return of the Jedi (1983)       2127  0.003697\n",
       "9                                        Fargo (1996)       2074  0.003605\n",
       "10                            Schindler's List (1993)       2071  0.003600\n",
       "11                   Shawshank Redemption, The (1994)       2046  0.003557\n",
       "12                  Terminator 2: Judgment Day (1991)       2044  0.003553\n",
       "13                              Godfather, The (1972)       1989  0.003457\n",
       "14                                  Braveheart (1995)       1977  0.003437\n",
       "15                         Princess Bride, The (1987)       1924  0.003344\n",
       "16                          Back to the Future (1985)       1910  0.003320\n",
       "17                         Shakespeare in Love (1998)       1877  0.003263\n",
       "18                           L.A. Confidential (1997)       1876  0.003261\n",
       "19                                Pulp Fiction (1994)       1770  0.003077"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Check 1: Popularity of positives ===\n",
    "pos = df[df[\"label\"] == 1]\n",
    "pop = pos[\"item_idx\"].value_counts()\n",
    "print(\"Pos rate:\", pos.shape[0], \"/\", df.shape[0], \"=\", pos.shape[0]/df.shape[0])\n",
    "\n",
    "top = pop.head(20).reset_index()\n",
    "top.columns = [\"item_idx\", \"pos_count\"]\n",
    "\n",
    "# map to title\n",
    "top[\"item_id\"] = top[\"item_idx\"].map(index_to_item_id)\n",
    "top = top.merge(movies[[\"item_id\",\"title\"]], on=\"item_id\", how=\"left\")\n",
    "top[\"share\"] = top[\"pos_count\"] / pop.sum()\n",
    "top[[\"title\",\"pos_count\",\"share\"]].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9909bd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling negatives (CLEAN): 100%|██████████| 6038/6038 [00:56<00:00, 106.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 580405\n",
      "Weight stats: count    580405.000000\n",
      "mean          0.123200\n",
      "std           0.211401\n",
      "min           0.008447\n",
      "25%           0.022938\n",
      "50%           0.042527\n",
      "75%           0.109336\n",
      "max           1.000000\n",
      "Name: sample_weight, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>item_idx</th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1189</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1926</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.141585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2841</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3616</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3682</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  gender  age  occupation  item_idx  genre  label  sample_weight\n",
       "0        1       0    0          10      1189      7      1       0.017814\n",
       "1        1       0    0          10      1926     10      0       0.141585\n",
       "2        1       0    0          10      2841      7      0       1.000000\n",
       "3        1       0    0          10      3616      4      0       0.032127\n",
       "4        1       0    0          10      3682      2      0       0.016396"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell 5 (REPLACED) — Sampling sạch: random pos + no false negatives + pop-aware negatives + sample_weight ===\n",
    "\n",
    "# FULL positives per user (từ df gốc, chưa bị truncation)\n",
    "user_pos_items = df[df[\"label\"]==1].groupby(\"user_id\")[\"item_idx\"].apply(set).to_dict()\n",
    "\n",
    "all_item_indices = np.array(sorted(item_id_to_index.values()), dtype=np.int64)\n",
    "all_item_set = set(all_item_indices.tolist())\n",
    "\n",
    "# ---- Popularity stats on FULL positives ----\n",
    "pop_by_item = np.zeros(len(item_id_to_index), dtype=np.int64)\n",
    "pos_counts = df[df[\"label\"]==1][\"item_idx\"].value_counts()\n",
    "for idx, cnt in pos_counts.items():\n",
    "    pop_by_item[int(idx)] = int(cnt)\n",
    "\n",
    "# Weight per item: w_i = 1 / (1 + pop(i))^beta\n",
    "POP_BETA = globals().get(\"POP_BETA\", 0.6)\n",
    "item_weight = 1.0 / np.power(1.0 + pop_by_item.astype(np.float32), POP_BETA)\n",
    "\n",
    "# ---- genre_by_item lookup (index -> genre_enc) ----\n",
    "genre_by_item = np.zeros(len(item_id_to_index), dtype=np.int32)\n",
    "for _, r in movies.iterrows():\n",
    "    idx = item_id_to_index[int(r[\"item_id\"])]\n",
    "    genre_by_item[idx] = int(r[\"genre_enc\"])\n",
    "\n",
    "# ---- pop-aware neg distribution: p(i) ∝ (pop(i)+1)^alpha ----\n",
    "NEG_POP_ALPHA = globals().get(\"NEG_POP_ALPHA\", 0.75)\n",
    "neg_base = np.power(pop_by_item.astype(np.float32) + 1.0, NEG_POP_ALPHA)\n",
    "\n",
    "NEG_POP_MIX = globals().get(\"NEG_POP_MIX\", 0.5)\n",
    "\n",
    "def sample_negatives_for_user(uid: int, num_neg: int, full_pos_set: set[int]):\n",
    "    \"\"\"Sample negatives excluding FULL positives of this user.\"\"\"\n",
    "    pool = np.array(sorted(all_item_set - full_pos_set), dtype=np.int64)\n",
    "    if len(pool) == 0:\n",
    "        pool = all_item_indices\n",
    "\n",
    "    n_pop = int(np.ceil(num_neg * NEG_POP_MIX))\n",
    "    n_uni = num_neg - n_pop\n",
    "\n",
    "    # pop-based\n",
    "    w = neg_base[pool]\n",
    "    w_sum = float(w.sum())\n",
    "    if w_sum <= 0:\n",
    "        pop_samples = np.random.choice(pool, size=n_pop, replace=(len(pool) < n_pop))\n",
    "    else:\n",
    "        p = w / w_sum\n",
    "        pop_samples = np.random.choice(pool, size=n_pop, replace=(len(pool) < n_pop), p=p)\n",
    "\n",
    "    # uniform\n",
    "    uni_samples = np.random.choice(pool, size=n_uni, replace=(len(pool) < n_uni))\n",
    "\n",
    "    out = np.concatenate([pop_samples, uni_samples]).astype(np.int64)\n",
    "\n",
    "    # Nếu pool đủ lớn, ép unique để tránh trùng\n",
    "    if len(pool) >= num_neg:\n",
    "        out = np.unique(out)\n",
    "        while len(out) < num_neg:\n",
    "            extra = np.random.choice(pool, size=(num_neg - len(out)), replace=False)\n",
    "            out = np.unique(np.concatenate([out, extra]))\n",
    "        out = out[:num_neg]\n",
    "    else:\n",
    "        # pool nhỏ, cho phép replace\n",
    "        if len(out) < num_neg:\n",
    "            extra = np.random.choice(pool, size=(num_neg - len(out)), replace=True)\n",
    "            out = np.concatenate([out, extra])[:num_neg]\n",
    "\n",
    "    return out.tolist()\n",
    "\n",
    "def build_training_rows(df_pos, neg_per_pos=4, max_samples_per_user=None):\n",
    "    rows = []\n",
    "    grouped = df_pos.groupby(\"user_id\")\n",
    "\n",
    "    for uid, g in tqdm(grouped, desc=\"Sampling negatives (CLEAN)\"):\n",
    "        # FULL positives set for this user (chống false negative)\n",
    "        full_pos_set = user_pos_items.get(uid, set())\n",
    "\n",
    "        # pos_items candidates từ df_pos (các row label==1), nhưng nếu giới hạn thì random\n",
    "        pos_items = g[\"item_idx\"].values.astype(np.int64)\n",
    "\n",
    "        if max_samples_per_user and len(pos_items) > max_samples_per_user:\n",
    "            pos_items = np.random.choice(pos_items, size=max_samples_per_user, replace=False)\n",
    "\n",
    "        pos_items = pos_items.tolist()\n",
    "\n",
    "        # user features\n",
    "        gender = int(g[\"gender_enc\"].iloc[0])\n",
    "        age    = int(g[\"age_enc\"].iloc[0])\n",
    "        occ    = int(g[\"occupation_enc\"].iloc[0])\n",
    "\n",
    "        for pos in pos_items:\n",
    "            # Positive row\n",
    "            rows.append([uid, gender, age, occ,\n",
    "                         int(pos), int(genre_by_item[int(pos)]),\n",
    "                         1, float(item_weight[int(pos)])])\n",
    "\n",
    "            # Negatives excluding FULL positives\n",
    "            negs = sample_negatives_for_user(uid, neg_per_pos, full_pos_set)\n",
    "            for neg in negs:\n",
    "                rows.append([uid, gender, age, occ,\n",
    "                             int(neg), int(genre_by_item[int(neg)]),\n",
    "                             0, float(item_weight[int(neg)])])\n",
    "\n",
    "    return rows\n",
    "\n",
    "df_pos = df[df[\"label\"]==1]\n",
    "rows = build_training_rows(df_pos, NEG_PER_POS, MAX_SAMPLES_PER_USER)\n",
    "\n",
    "train_df = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"user_id\",\"gender\",\"age\",\"occupation\",\"item_idx\",\"genre\",\"label\",\"sample_weight\"]\n",
    ")\n",
    "\n",
    "print(\"Train rows:\", len(train_df))\n",
    "print(\"Weight stats:\", train_df[\"sample_weight\"].describe())\n",
    "train_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1d3cf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top items in FULL positives:\n",
      "[2789, 257, 1178, 1959, 1180, 589, 2502, 2693, 1192, 604]\n",
      "\n",
      "Top items in FIRST-20 positives per user:\n",
      "[589, 585, 0, 2928, 2789, 1250, 1245, 1239, 3724, 257]\n"
     ]
    }
   ],
   "source": [
    "# === Check 2: Are we truncating positives in a biased way? ===\n",
    "tmp = df[df[\"label\"]==1].copy()\n",
    "# xem 20 item đầu tiên theo thứ tự xuất hiện của mỗi user\n",
    "first20 = tmp.groupby(\"user_id\").head(20)\n",
    "first20_pop = first20[\"item_idx\"].value_counts().head(20)\n",
    "\n",
    "full_pop = tmp[\"item_idx\"].value_counts().head(20)\n",
    "\n",
    "print(\"Top items in FULL positives:\")\n",
    "print(full_pop.head(10).index.tolist())\n",
    "\n",
    "print(\"\\nTop items in FIRST-20 positives per user:\")\n",
    "print(first20_pop.head(10).index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d7f7077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negatives (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "# === Correct false-negative check for NEW pipeline ===\n",
    "uid = list(user_pos_items.keys())[0]\n",
    "full_pos_set = user_pos_items[uid]\n",
    "\n",
    "# pool negatives theo pipeline mới\n",
    "pool = set(all_item_indices.tolist()) - full_pos_set\n",
    "print(\"False negatives (should be 0):\", len(pool.intersection(full_pos_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0df7e4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 470127 52237 58041\n"
     ]
    }
   ],
   "source": [
    "# Split theo hàng (không theo user), đơn giản cho demo\n",
    "train_part, test_part = train_test_split(train_df, test_size=TEST_SIZE, random_state=SEED, stratify=train_df[\"label\"])\n",
    "train_part, val_part  = train_test_split(train_part, test_size=VAL_SIZE, random_state=SEED, stratify=train_part[\"label\"])\n",
    "\n",
    "print(\"Split sizes:\", len(train_part), len(val_part), len(test_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04639415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gender': torch.Size([1024]), 'age': torch.Size([1024]), 'occupation': torch.Size([1024]), 'item_id': torch.Size([1024]), 'genre': torch.Size([1024])} torch.Size([1024]) torch.Size([1024]) w[0:5]= tensor([0.0884, 1.0000, 0.0849, 0.0503, 0.0980])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(470127, 52237, 58041)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FMTrainDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.gender = df[\"gender\"].values.astype(np.int64)\n",
    "        self.age = df[\"age\"].values.astype(np.int64)\n",
    "        self.occ = df[\"occupation\"].values.astype(np.int64)\n",
    "        self.item = df[\"item_idx\"].values.astype(np.int64)\n",
    "        self.genre = df[\"genre\"].values.astype(np.int64)\n",
    "        self.label = df[\"label\"].values.astype(np.float32)\n",
    "        self.w = df[\"sample_weight\"].values.astype(np.float32)  # <--- NEW\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"gender\": self.gender[idx],\n",
    "            \"age\": self.age[idx],\n",
    "            \"occupation\": self.occ[idx],\n",
    "            \"item_id\": self.item[idx],\n",
    "            \"genre\": self.genre[idx],\n",
    "            \"label\": self.label[idx],\n",
    "            \"weight\": self.w[idx],  # <--- NEW\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    out = {k: [] for k in [\"gender\",\"age\",\"occupation\",\"item_id\",\"genre\",\"label\",\"weight\"]}\n",
    "    for b in batch:\n",
    "        for k in out:\n",
    "            out[k].append(b[k])\n",
    "\n",
    "    for k in [\"gender\",\"age\",\"occupation\",\"item_id\",\"genre\"]:\n",
    "        out[k] = torch.tensor(out[k], dtype=torch.long)\n",
    "\n",
    "    labels = torch.tensor(out.pop(\"label\"), dtype=torch.float32)\n",
    "    weights = torch.tensor(out.pop(\"weight\"), dtype=torch.float32)  # <--- NEW\n",
    "\n",
    "    return out, labels, weights\n",
    "\n",
    "train_ds = FMTrainDataset(train_part)\n",
    "val_ds   = FMTrainDataset(val_part)\n",
    "test_ds  = FMTrainDataset(test_part)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "b, y, w = next(iter(train_loader))\n",
    "print({k: b[k].shape for k in b}, y.shape, w.shape, \"w[0:5]=\", w[:5])\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, collate_fn=collate_fn, pin_memory=False, persistent_workers=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, collate_fn=collate_fn, pin_memory=False, persistent_workers=False\n",
    ")\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06fa055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, field_dims: dict, embed_dim=16, mlp_dims=[128,64], dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fields = list(field_dims.keys())\n",
    "\n",
    "        # Embedding cho FM/DNN (kích thước d)\n",
    "        self.emb = nn.ModuleDict({\n",
    "            k: nn.Embedding(field_dims[k], embed_dim) for k in self.fields\n",
    "        })\n",
    "        # Linear term: mỗi field một Embedding dim=1\n",
    "        self.lin = nn.ModuleDict({\n",
    "            k: nn.Embedding(field_dims[k], 1) for k in self.fields\n",
    "        })\n",
    "\n",
    "        # DNN\n",
    "        in_dim = embed_dim * len(self.fields)\n",
    "        layers = []\n",
    "        d = in_dim\n",
    "        for h in mlp_dims:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            d = h\n",
    "        self.dnn = nn.Sequential(*layers)\n",
    "        self.dnn_out = nn.Linear(d, 1)\n",
    "\n",
    "        # init\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for emb in self.emb.values():\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "        for l in self.lin.values():\n",
    "            nn.init.zeros_(l.weight.data)\n",
    "\n",
    "    def forward(self, x: dict):\n",
    "        \"\"\"\n",
    "        x: dict tensor Long: [\"gender\",\"age\",\"occupation\",\"item_id\",\"genre\"] shape [B]\n",
    "        Output: logits [B, 1]\n",
    "        \"\"\"\n",
    "        # Linear\n",
    "        lin_terms = [self.lin[k](x[k]) for k in self.fields]  # list [B,1]\n",
    "        lin = torch.stack(lin_terms, dim=1).sum(dim=1)        # [B,1]\n",
    "\n",
    "        # Embeddings\n",
    "        embs = [self.emb[k](x[k]) for k in self.fields]       # list [B,d]\n",
    "        E = torch.stack(embs, dim=1)                          # [B,F,d]\n",
    "\n",
    "        # FM 2nd order: 0.5 * (sum^2 - sum of squares)\n",
    "        sum_of_emb = E.sum(dim=1)                             # [B,d]\n",
    "        sum_of_emb_square = sum_of_emb * sum_of_emb           # [B,d]\n",
    "        square_of_emb = E * E                                 # [B,F,d]\n",
    "        square_of_emb_sum = square_of_emb.sum(dim=1)          # [B,d]\n",
    "        fm = 0.5 * (sum_of_emb_square - square_of_emb_sum)    # [B,d]\n",
    "        fm_logit = fm.sum(dim=1, keepdim=True)                # [B,1]\n",
    "\n",
    "        # DNN\n",
    "        dnn_in = torch.cat(embs, dim=1)                       # [B, F*d]\n",
    "        dnn_hidden = self.dnn(dnn_in)                         # [B, H]\n",
    "        dnn_logit = self.dnn_out(dnn_hidden)                  # [B,1]\n",
    "\n",
    "        logits = lin + fm_logit + dnn_logit                   # [B,1]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec915051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.085516, 'M params')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_items  = len(item_id_to_index)\n",
    "num_genres = len(genre_to_index)\n",
    "\n",
    "field_dims = {\n",
    "    \"gender\": 2,\n",
    "    \"age\": 7,\n",
    "    \"occupation\": 21,\n",
    "    \"item_id\": num_items,\n",
    "    \"genre\": num_genres\n",
    "}\n",
    "\n",
    "model = DeepFM(field_dims, embed_dim=EMBED_DIM, mlp_dims=MLP_DIMS, dropout=DROPOUT).to(DEVICE)\n",
    "\n",
    "# IMPORTANT: reduction='none' để nhân sample_weight\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())/1e6, \"M params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64a9e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    for batch, labels, weights in loader:\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        logits = model(batch).squeeze(1)  # [B]\n",
    "        all_logits.append(logits.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    logits = np.concatenate(all_logits)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    probs = 1/(1+np.exp(-logits))\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "    ll = log_loss(labels, probs, labels=[0,1])\n",
    "\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = (preds == labels).mean()\n",
    "\n",
    "    return auc, ll, acc\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for bi, (batch, labels, weights) in enumerate(tqdm(loader, total=len(loader), desc=\"Train\")):\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        weights = weights.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch).squeeze(1)                     # [B]\n",
    "        per_sample = criterion(logits, labels)               # [B] because reduction='none'\n",
    "        loss = (per_sample * weights).mean()                 # weighted mean\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        if (bi + 1) % 50 == 0:\n",
    "            print(f\"  step {bi+1}/{len(loader)}  loss={loss.item():.4f}\")\n",
    "\n",
    "    return running_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55e8bb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  12%|█▏        | 55/460 [00:01<00:07, 51.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 50/460  loss=0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  23%|██▎       | 106/460 [00:01<00:05, 67.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 100/460  loss=0.0239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  35%|███▌      | 162/460 [00:02<00:04, 74.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 150/460  loss=0.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  46%|████▌     | 211/460 [00:03<00:03, 76.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 200/460  loss=0.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  57%|█████▋    | 260/460 [00:03<00:02, 76.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 250/460  loss=0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  67%|██████▋   | 309/460 [00:04<00:01, 77.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 300/460  loss=0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  78%|███████▊  | 358/460 [00:05<00:01, 72.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 350/460  loss=0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  90%|█████████ | 414/460 [00:05<00:00, 74.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 400/460  loss=0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 460/460 [00:06<00:00, 69.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 450/460  loss=0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] TrainLoss=0.0232 | ValAUC=0.7651 | ValLogLoss=0.4265 | ValAcc=0.8053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  13%|█▎        | 59/460 [00:00<00:06, 62.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 50/460  loss=0.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  23%|██▎       | 107/460 [00:01<00:04, 71.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 100/460  loss=0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  35%|███▌      | 163/460 [00:02<00:04, 73.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 150/460  loss=0.0183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  46%|████▌     | 211/460 [00:03<00:03, 72.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 200/460  loss=0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  56%|█████▋    | 259/460 [00:03<00:02, 73.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 250/460  loss=0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  67%|██████▋   | 308/460 [00:04<00:02, 75.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 300/460  loss=0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  77%|███████▋  | 356/460 [00:04<00:01, 74.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 350/460  loss=0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  90%|████████▉ | 413/460 [00:05<00:00, 74.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 400/460  loss=0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 460/460 [00:06<00:00, 72.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 450/460  loss=0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02] TrainLoss=0.0178 | ValAUC=0.7738 | ValLogLoss=0.4204 | ValAcc=0.8074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  13%|█▎        | 62/460 [00:00<00:05, 74.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 50/460  loss=0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  24%|██▍       | 110/460 [00:01<00:04, 73.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 100/460  loss=0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  35%|███▍      | 159/460 [00:02<00:03, 76.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 150/460  loss=0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  45%|████▌     | 207/460 [00:02<00:03, 72.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 200/460  loss=0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  56%|█████▌    | 256/460 [00:03<00:02, 75.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 250/460  loss=0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  68%|██████▊   | 312/460 [00:04<00:01, 74.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 300/460  loss=0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  78%|███████▊  | 358/460 [00:04<00:01, 67.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 350/460  loss=0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  90%|████████▉ | 413/460 [00:05<00:00, 72.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 400/460  loss=0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 460/460 [00:06<00:00, 71.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 450/460  loss=0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03] TrainLoss=0.0173 | ValAUC=0.7773 | ValLogLoss=0.4182 | ValAcc=0.8092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  13%|█▎        | 59/460 [00:00<00:05, 67.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 50/460  loss=0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  23%|██▎       | 105/460 [00:01<00:05, 69.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 100/460  loss=0.0181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  34%|███▎      | 155/460 [00:02<00:04, 66.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 150/460  loss=0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  45%|████▌     | 207/460 [00:03<00:03, 63.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 200/460  loss=0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  56%|█████▋    | 259/460 [00:04<00:03, 64.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 250/460  loss=0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  67%|██████▋   | 309/460 [00:04<00:02, 61.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 300/460  loss=0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  78%|███████▊  | 360/460 [00:05<00:01, 64.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 350/460  loss=0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  90%|████████▉ | 412/460 [00:06<00:00, 69.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 400/460  loss=0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████▉| 459/460 [00:07<00:00, 66.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 450/460  loss=0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 460/460 [00:07<00:00, 64.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04] TrainLoss=0.0170 | ValAUC=0.7813 | ValLogLoss=0.4175 | ValAcc=0.8096\n",
      "[TEST] AUC=0.7841 | LogLoss=0.4156 | Acc=0.8094\n"
     ]
    }
   ],
   "source": [
    "best_val_auc = -1\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_auc, val_ll, val_acc = evaluate(model, val_loader)\n",
    "    print(f\"[Epoch {epoch:02d}] TrainLoss={tr_loss:.4f} | ValAUC={val_auc:.4f} | ValLogLoss={val_ll:.4f} | ValAcc={val_acc:.4f}\")\n",
    "\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "# Load best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "test_auc, test_ll, test_acc = evaluate(model, test_loader)\n",
    "print(f\"[TEST] AUC={test_auc:.4f} | LogLoss={test_ll:.4f} | Acc={test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "556c7d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top popular items: [2789, 257, 1178, 1959, 1180, 589, 2502, 2693, 1192, 604, 523, 315, 585, 847, 108, 1179, 1250, 2327, 1575, 293]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>avg_prob</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2858</td>\n",
       "      <td>0.671588</td>\n",
       "      <td>American Beauty (1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>260</td>\n",
       "      <td>0.618282</td>\n",
       "      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2028</td>\n",
       "      <td>0.613051</td>\n",
       "      <td>Saving Private Ryan (1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>593</td>\n",
       "      <td>0.588661</td>\n",
       "      <td>Silence of the Lambs, The (1991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1210</td>\n",
       "      <td>0.577288</td>\n",
       "      <td>Star Wars: Episode VI - Return of the Jedi (1983)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1196</td>\n",
       "      <td>0.575563</td>\n",
       "      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2571</td>\n",
       "      <td>0.573567</td>\n",
       "      <td>Matrix, The (1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>110</td>\n",
       "      <td>0.571739</td>\n",
       "      <td>Braveheart (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2762</td>\n",
       "      <td>0.564832</td>\n",
       "      <td>Sixth Sense, The (1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1198</td>\n",
       "      <td>0.552478</td>\n",
       "      <td>Raiders of the Lost Ark (1981)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  avg_prob                                              title\n",
       "0     2858  0.671588                             American Beauty (1999)\n",
       "1      260  0.618282          Star Wars: Episode IV - A New Hope (1977)\n",
       "2     2028  0.613051                         Saving Private Ryan (1998)\n",
       "3      593  0.588661                   Silence of the Lambs, The (1991)\n",
       "4     1210  0.577288  Star Wars: Episode VI - Return of the Jedi (1983)\n",
       "5     1196  0.575563  Star Wars: Episode V - The Empire Strikes Back...\n",
       "6     2571  0.573567                                 Matrix, The (1999)\n",
       "7      110  0.571739                                  Braveheart (1995)\n",
       "8     2762  0.564832                            Sixth Sense, The (1999)\n",
       "9     1198  0.552478                     Raiders of the Lost Ark (1981)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Check 4: Global winners (average predicted prob over random users) ===\n",
    "@torch.no_grad()\n",
    "def avg_item_score_over_users(model, item_ids, sample_users=2000):\n",
    "    model.eval()\n",
    "    uids = np.random.choice(users[\"user_id\"].values, size=sample_users, replace=False)\n",
    "    out = []\n",
    "    for iid in item_ids:\n",
    "        # build batch for many users with fixed item\n",
    "        g = users.set_index(\"user_id\").loc[uids, \"gender_enc\"].values.astype(np.int64)\n",
    "        a = users.set_index(\"user_id\").loc[uids, \"age_enc\"].values.astype(np.int64)\n",
    "        o = users.set_index(\"user_id\").loc[uids, \"occupation_enc\"].values.astype(np.int64)\n",
    "\n",
    "        item_idx = item_id_to_index[int(iid)]\n",
    "        ge = int(movies.loc[movies[\"item_id\"]==iid, \"genre_enc\"].iloc[0])\n",
    "\n",
    "        batch = {\n",
    "            \"gender\": torch.tensor(g, device=DEVICE),\n",
    "            \"age\": torch.tensor(a, device=DEVICE),\n",
    "            \"occupation\": torch.tensor(o, device=DEVICE),\n",
    "            \"item_id\": torch.full((sample_users,), item_idx, dtype=torch.long, device=DEVICE),\n",
    "            \"genre\": torch.full((sample_users,), ge, dtype=torch.long, device=DEVICE),\n",
    "        }\n",
    "        logits = model(batch).view(-1).detach().cpu().numpy()\n",
    "        prob = 1/(1+np.exp(-logits))\n",
    "        out.append((iid, prob.mean()))\n",
    "    return out\n",
    "\n",
    "# test top-pop items\n",
    "top_item_ids = (df[df[\"label\"]==1][\"item_idx\"].value_counts().head(20).index\n",
    "                .to_series().map(index_to_item_id).tolist())\n",
    "print(\"Top popular items:\", df[df[\"label\"]==1][\"item_idx\"]\n",
    "                .value_counts()\n",
    "                .head(20).index.tolist())\n",
    "\n",
    "avg_scores = avg_item_score_over_users(model, top_item_ids, sample_users=2000)\n",
    "avg_scores = sorted(avg_scores, key=lambda x: -x[1])\n",
    "\n",
    "res = pd.DataFrame(avg_scores, columns=[\"item_id\",\"avg_prob\"])\n",
    "res = res.merge(movies[[\"item_id\",\"title\"]], on=\"item_id\", how=\"left\")\n",
    "res.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a92c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Saved: artifacts\\deepfm_pytorch.pth\n",
      "[✓] Saved: artifacts\\item_emb.npy (3883, 16)\n",
      "[✓] Saved mappings to artifacts/\n"
     ]
    }
   ],
   "source": [
    "# 1) Lưu model (state_dict)\n",
    "pth_path = os.path.join(ART_DIR, \"deepfm_pytorch.pth\")\n",
    "torch.save(model.state_dict(), pth_path)\n",
    "print(\"[✓] Saved:\", pth_path)\n",
    "\n",
    "# 2) Xuất item embedding để dùng FAISS & /similar_items\n",
    "item_emb_weight = model.emb[\"item_id\"].weight.detach().cpu().numpy().astype(\"float32\")\n",
    "np.save(os.path.join(ART_DIR, \"item_emb.npy\"), item_emb_weight)\n",
    "print(\"[✓] Saved:\", os.path.join(ART_DIR, \"item_emb.npy\"), item_emb_weight.shape)\n",
    "\n",
    "# 3) Lưu mapping\n",
    "with open(os.path.join(ART_DIR, \"item_id_to_index.json\"), \"w\") as f:\n",
    "    json.dump({int(k): int(v) for k, v in item_id_to_index.items()}, f)\n",
    "with open(os.path.join(ART_DIR, \"index_to_item_id.json\"), \"w\") as f:\n",
    "    json.dump({int(k): int(v) for k, v in index_to_item_id.items()}, f)\n",
    "print(\"[✓] Saved mappings to artifacts/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed8a1b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 item_idx: [2890 2502  293   49 3091 1195  585 1192  257   31]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Twelve Monkeys (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Usual Suspects, The (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>260</td>\n",
       "      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>296</td>\n",
       "      <td>Pulp Fiction (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>589</td>\n",
       "      <td>Terminator 2: Judgment Day (1991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>1210</td>\n",
       "      <td>Star Wars: Episode VI - Return of the Jedi (1983)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1213</td>\n",
       "      <td>GoodFellas (1990)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>2571</td>\n",
       "      <td>Matrix, The (1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>2959</td>\n",
       "      <td>Fight Club (1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>3160</td>\n",
       "      <td>Magnolia (1999)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id                                              title\n",
       "31         32                              Twelve Monkeys (1995)\n",
       "49         50                         Usual Suspects, The (1995)\n",
       "257       260          Star Wars: Episode IV - A New Hope (1977)\n",
       "293       296                                Pulp Fiction (1994)\n",
       "585       589                  Terminator 2: Judgment Day (1991)\n",
       "1192     1210  Star Wars: Episode VI - Return of the Jedi (1983)\n",
       "1195     1213                                  GoodFellas (1990)\n",
       "2502     2571                                 Matrix, The (1999)\n",
       "2890     2959                                  Fight Club (1999)\n",
       "3091     3160                                    Magnolia (1999)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ví dụ: tạo vector u giả (mean của một vài item hành động) rồi chấm điểm\n",
    "# Trong thực tế bạn sẽ tối ưu u từ lịch sử user hoặc cập nhật online theo event.\n",
    "\n",
    "# Lấy 10 item ngẫu nhiên\n",
    "sample_idx = np.random.choice(len(item_id_to_index), size=10, replace=False)\n",
    "E = item_emb_weight[sample_idx]  # [10, d]\n",
    "u = E.mean(axis=0)               # [d]\n",
    "u = u / (np.linalg.norm(u) + 1e-6)\n",
    "\n",
    "scores = item_emb_weight @ u     # [num_items]\n",
    "topk = 10\n",
    "top_idx = np.argpartition(-scores, topk)[:topk]\n",
    "top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "print(\"Top-10 item_idx:\", top_idx)\n",
    "\n",
    "# map sang item_id & title\n",
    "inv_map = {v:k for k, v in item_id_to_index.items()}\n",
    "top_item_ids = [inv_map[i] for i in top_idx]\n",
    "top_titles = movies[movies[\"item_id\"].isin(top_item_ids)][[\"item_id\",\"title\"]]\n",
    "top_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe8254bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.4\n",
      "  Using cached numpy-1.24.4-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.24.4-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.4.0\n",
      "    Uninstalling numpy-2.4.0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Python311\\Lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\python311\\\\scripts\\\\f2py.exe'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.24.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c20473d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\python311\\lib\\site-packages (1.12.0)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.13.2-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting numpy<3.0,>=1.25.0 (from faiss-cpu)\n",
      "  Using cached numpy-2.4.0-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\this pc\\appdata\\roaming\\python\\python311\\site-packages (from faiss-cpu) (24.2)\n",
      "Using cached faiss_cpu-1.13.2-cp311-cp311-win_amd64.whl (18.9 MB)\n",
      "Using cached numpy-2.4.0-cp311-cp311-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy, faiss-cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Python311\\Lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Python311\\\\Scripts\\\\f2py.exe' -> 'C:\\\\Python311\\\\Scripts\\\\f2py.exe.deleteme'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55ae8946",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot load module more than once per process",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Chỉ cần nếu bạn muốn test Similar ngay trong notebook\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# !pip install faiss-cpu\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m      5\u001b[0m emb \u001b[38;5;241m=\u001b[39m item_emb_weight\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      6\u001b[0m faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(emb)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\faiss\\__init__.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# We import * so that the symbol foo can be accessed as faiss.foo.\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# additional wrappers\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m class_wrappers\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\faiss\\loader.py:97\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopt_env_variable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not set, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m     96\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mso let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms pick the instruction set according to the current CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m     instruction_sets \u001b[38;5;241m=\u001b[39m \u001b[43msupported_instruction_sets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopt_level\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an instruction set.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\faiss\\loader.py:55\u001b[0m, in \u001b[0;36msupported_instruction_sets\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Version(numpy\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.19\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# use private API as next-best thing until numpy/numpy#18058 is solved\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __cpu_features__\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# __cpu_features__ is a dictionary with CPU features\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# as keys, and True / False as values\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     supported \u001b[38;5;241m=\u001b[39m {k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m __cpu_features__\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v}\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\numpy\\_core\\__init__.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m         env_added\u001b[38;5;241m.\u001b[39mappend(envkey)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\numpy\\_core\\multiarray.py:11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCreate the numpy._core.multiarray namespace for backward compatibility.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mIn v1.16 the multiarray and umath c-extension modules were merged into\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath, overrides\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# These imports are needed for backward compatibility,\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# do not change them. issue gh-15518\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# _get_ndarray_c_version is semi-public, on purpose not added to __all__\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot load module more than once per process"
     ]
    }
   ],
   "source": [
    "# Chỉ cần nếu bạn muốn test Similar ngay trong notebook\n",
    "# !pip install faiss-cpu\n",
    "\n",
    "import faiss\n",
    "emb = item_emb_weight.copy()\n",
    "faiss.normalize_L2(emb)\n",
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "\n",
    "def similar_items_by_item_id(raw_item_id, k=10):\n",
    "    idx = item_id_to_index[int(raw_item_id)]\n",
    "    q = emb[idx:idx+1]\n",
    "    D, I = index.search(q, k+1)\n",
    "    I = I[0].tolist()\n",
    "    D = D[0].tolist()\n",
    "    out = []\n",
    "    for d, i in zip(D, I):\n",
    "        if i == idx: \n",
    "            continue\n",
    "        iid = index_to_item_id[i]\n",
    "        title = movies.loc[movies[\"item_id\"]==iid, \"title\"].values\n",
    "        title = title[0] if len(title) else str(iid)\n",
    "        out.append({\"item_id\": int(iid), \"title\": title, \"score\": float(d)})\n",
    "        if len(out) == k: break\n",
    "    return out\n",
    "\n",
    "# Demo với 1 item bất kỳ:\n",
    "some_item = int(movies[\"item_id\"].sample(1, random_state=SEED).iloc[0])\n",
    "similar_items_by_item_id(some_item, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helpers + Patch (FINAL) — dùng đúng model.fields ===\n",
    "import numpy as np, torch\n",
    "\n",
    "# Pool item và map user->set item đã like (rating >= 4)\n",
    "ALL_ITEM_IDS = np.array(sorted(movies[\"item_id\"].unique()))\n",
    "watched_by_user = ratings[ratings[\"rating\"] >= 4].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "\n",
    "def sample_negatives_for_user(user_id: int, num_neg: int = 99):\n",
    "    pos_set = watched_by_user.get(user_id, set())\n",
    "    pool = (ALL_ITEM_IDS if len(pos_set) == 0\n",
    "            else np.setdiff1d(ALL_ITEM_IDS, np.fromiter(pos_set, dtype=int), assume_unique=True))\n",
    "    if len(pool) == 0: pool = ALL_ITEM_IDS\n",
    "    replace = len(pool) < num_neg\n",
    "    return np.random.choice(pool, size=num_neg, replace=replace).tolist()\n",
    "\n",
    "def _tensors_for_user_items(user_id: int, item_ids: list[int]):\n",
    "    \"\"\"Chuẩn bị (u,i,g,a,o,ge) tensors theo đúng thứ tự item_ids.\"\"\"\n",
    "    # user feats\n",
    "    urow = users.loc[users[\"user_id\"] == user_id, [\"gender_enc\",\"age_enc\",\"occupation_enc\"]]\n",
    "    if urow.empty:\n",
    "        raise ValueError(f\"User {user_id} không tồn tại trong users df\")\n",
    "    g = int(urow[\"gender_enc\"].values[0])\n",
    "    a = int(urow[\"age_enc\"].values[0])\n",
    "    o = int(urow[\"occupation_enc\"].values[0])\n",
    "\n",
    "    # item feats (theo đúng thứ tự item_ids)\n",
    "    msub = movies.loc[movies[\"item_id\"].isin(item_ids), [\"item_id\",\"genre_enc\"]].copy()\n",
    "    msub = msub.set_index(\"item_id\").reindex(item_ids).reset_index()\n",
    "    item_idx = [item_id_to_index[int(i)] for i in msub[\"item_id\"].values]\n",
    "    genre_enc = msub[\"genre_enc\"].astype(int).values\n",
    "\n",
    "    n = len(item_ids)\n",
    "    uT = torch.full((n,), int(user_id), dtype=torch.long, device=DEVICE)\n",
    "    iT = torch.tensor(item_idx, dtype=torch.long, device=DEVICE)\n",
    "    gT = torch.full((n,), g, dtype=torch.long, device=DEVICE)\n",
    "    aT = torch.full((n,), a, dtype=torch.long, device=DEVICE)\n",
    "    oT = torch.full((n,), o, dtype=torch.long, device=DEVICE)\n",
    "    geT= torch.tensor(genre_enc, dtype=torch.long, device=DEVICE)\n",
    "    return uT, iT, gT, aT, oT, geT\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_candidates(user_id: int, item_ids: list[int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Gọi model theo đúng chữ ký dict dựa vào model.fields.\n",
    "    Ví dụ nếu model.fields = [\"gender\",\"age\",\"occupation\",\"item_id\",\"genre\"],\n",
    "    ta sẽ cung cấp một dict với đúng các khóa đó.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    uT, iT, gT, aT, oT, geT = _tensors_for_user_items(user_id, item_ids)\n",
    "\n",
    "    # Lấy danh sách khóa model yêu cầu\n",
    "    fields = getattr(model, \"fields\", None)\n",
    "    if fields is None:\n",
    "        # Fallback phổ biến: dùng tên khóa tiêu chuẩn\n",
    "        fields = [\"user_id\",\"item_id\",\"gender\",\"age\",\"occupation\",\"genre\"]\n",
    "\n",
    "    batch = {}\n",
    "    for k in fields:\n",
    "        lk = k.lower()\n",
    "        if lk in (\"user\", \"user_id\", \"uid\"):\n",
    "            batch[k] = uT\n",
    "        elif lk in (\"item\", \"item_id\", \"item_idx\", \"iid\"):\n",
    "            # DÙ tên là item_id nhưng thực chất model thường expect chỉ số embedding (item_idx)\n",
    "            batch[k] = iT\n",
    "        elif lk in (\"gender\", \"sex\"):\n",
    "            batch[k] = gT\n",
    "        elif lk == \"age\":\n",
    "            batch[k] = aT\n",
    "        elif lk in (\"occupation\", \"occ\", \"job\"):\n",
    "            batch[k] = oT\n",
    "        elif lk in (\"genre\", \"genre_enc\"):\n",
    "            batch[k] = geT\n",
    "        else:\n",
    "            raise KeyError(f\"Không biết map khóa '{k}' trong model.fields -> cung cấp tensor nào\")\n",
    "\n",
    "    out = model(batch)          # forward(self, x: dict)\n",
    "    # Chuẩn về numpy 1D\n",
    "    out = out.detach().float().view(-1).cpu().numpy()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\this pc\\AppData\\Local\\Temp\\ipykernel_2020\\1679843186.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_pos = test_pos.groupby(\"user_id\", group_keys=False).apply(\n",
      "Eval Hit@10/NDCG@10: 100%|██████████| 5213/5213 [00:31<00:00, 165.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Users evaluated: 5213\n",
      "[TEST] Hit@10 = 0.7527 | NDCG@10 = 0.4955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate Hit@K & NDCG@K (REPLACED; place after Cell 15) ===\n",
    "# Nhớ chạy Cell 11 (load best model) trước cell này.\n",
    "\n",
    "K = TOPK if \"TOPK\" in globals() else 10\n",
    "NEG_PER_USER_EVAL = 99\n",
    "\n",
    "# Dùng test_df nếu đã alias ở Cell 15a; nếu không có thì dùng test_part\n",
    "_eval_source = test_part if \"test_df\" in globals() else test_part\n",
    "\n",
    "# Xác định cột item: ưu tiên item_id; nếu không có thì dùng item_idx và map lại\n",
    "if \"item_id\" in _eval_source.columns:\n",
    "    ITEM_COL = \"item_id\"\n",
    "elif \"item_idx\" in _eval_source.columns:\n",
    "    ITEM_COL = \"item_idx\"\n",
    "else:\n",
    "    raise RuntimeError(\"Không thấy cột item_id hoặc item_idx trong test set.\")\n",
    "\n",
    "# Lọc positive và lấy ngẫu nhiên 1 positive / user\n",
    "test_pos = _eval_source[_eval_source[\"label\"] == 1].copy()\n",
    "if len(test_pos) == 0:\n",
    "    raise RuntimeError(\"Không tìm thấy positive nào trong test set để đánh giá Hit@K/NDCG@K.\")\n",
    "\n",
    "test_pos = test_pos.groupby(\"user_id\", group_keys=False).apply(\n",
    "    lambda x: x.sample(n=1, random_state=SEED)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "hits, ndcgs = [], []\n",
    "\n",
    "# Dùng iterrows để truy cập theo tên cột ổn định\n",
    "for _, row in tqdm(test_pos.iterrows(), total=len(test_pos), desc=f\"Eval Hit@{K}/NDCG@{K}\"):\n",
    "    u = int(row[\"user_id\"])\n",
    "    if ITEM_COL == \"item_id\":\n",
    "        pos_item = int(row[\"item_id\"])\n",
    "    else:\n",
    "        # map item_idx -> item_id gốc\n",
    "        pos_item = int(index_to_item_id[int(row[\"item_idx\"])])\n",
    "\n",
    "    neg_items = sample_negatives_for_user(u, num_neg=NEG_PER_USER_EVAL)\n",
    "\n",
    "    # 1 positive + negatives (loại trùng)\n",
    "    candidates = [pos_item] + [it for it in neg_items if it != pos_item]\n",
    "    scores = score_candidates(u, candidates)\n",
    "\n",
    "    # Xếp hạng giảm dần\n",
    "    order = np.argsort(-scores)\n",
    "    ranked_items = np.array(candidates, dtype=int)[order]\n",
    "\n",
    "    # Hit@K\n",
    "    topk_items = ranked_items[:K]\n",
    "    hit = 1.0 if pos_item in topk_items else 0.0\n",
    "\n",
    "    # NDCG@K (IDCG=1). Nếu pos ngoài top-K => 0\n",
    "    r_idx = np.where(ranked_items == pos_item)[0]\n",
    "    if len(r_idx) == 0:\n",
    "        dcg_at_k = 0.0\n",
    "    else:\n",
    "        r = int(r_idx[0]) + 1  # 1-based\n",
    "        dcg_at_k = (1.0 / np.log2(r + 1)) if r <= K else 0.0\n",
    "    ndcg = dcg_at_k\n",
    "\n",
    "    hits.append(hit)\n",
    "    ndcgs.append(ndcg)\n",
    "\n",
    "print(f\"[TEST] Users evaluated: {len(test_pos)}\")\n",
    "print(f\"[TEST] Hit@{K} = {np.mean(hits):.4f} | NDCG@{K} = {np.mean(ndcgs):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
